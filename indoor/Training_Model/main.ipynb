{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f329170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "infoExcel = \"data/MeasureData.xlsx\"\n",
    "fileData = pd.read_excel(infoExcel,sheet_name=\"data\", header=None)\n",
    "fileID = pd.read_excel(infoExcel,sheet_name=\"ID\", header=None)\n",
    "size = fileData.shape\n",
    "startX = size[0]\n",
    "startY = size[1]\n",
    "c = 0\n",
    "print(startX)\n",
    "print(startY)\n",
    "for x in range(0,startX):\n",
    "    for y in range(0,startY):\n",
    "        if pd.isnull(fileID.iloc[x][y]):\n",
    "            continue\n",
    "        c = c+1\n",
    "print(fileID)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28228402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c13d508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:22: conv2d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:23: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:35: flatten (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:40: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:43: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Training...\n",
      "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n",
      "batch 0: average train_loss=106.553085 \n",
      "training------0, loss: 106.003810\n",
      "batch 100: average train_loss=104.246056 \n",
      "training------100, loss: 103.628658\n",
      "batch 200: average train_loss=102.109413 \n",
      "training------200, loss: 101.442266\n",
      "batch 300: average train_loss=100.144394 \n",
      "training------300, loss: 99.471823\n",
      "batch 400: average train_loss=98.293373 \n",
      "training------400, loss: 97.645357\n",
      "batch 500: average train_loss=96.505264 \n",
      "training------500, loss: 95.894357\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "batch 600: average train_loss=94.793121 \n",
      "training------600, loss: 94.226618\n",
      "batch 700: average train_loss=93.174789 \n",
      "training------700, loss: 92.658037\n",
      "batch 800: average train_loss=91.616096 \n",
      "training------800, loss: 91.152283\n",
      "batch 900: average train_loss=90.077904 \n",
      "training------900, loss: 89.661852\n",
      "batch 1000: average train_loss=88.556053 \n",
      "training------1000, loss: 88.187415\n",
      "batch 1100: average train_loss=87.045044 \n",
      "training------1100, loss: 86.727464\n",
      "batch 1200: average train_loss=85.530273 \n",
      "training------1200, loss: 85.261983\n",
      "batch 1300: average train_loss=84.012413 \n",
      "training------1300, loss: 83.795227\n",
      "batch 1400: average train_loss=82.496445 \n",
      "training------1400, loss: 82.330423\n",
      "batch 1500: average train_loss=80.976143 \n",
      "training------1500, loss: 80.861133\n",
      "batch 1600: average train_loss=79.444038 \n",
      "training------1600, loss: 79.378230\n",
      "batch 1700: average train_loss=77.902550 \n",
      "training------1700, loss: 77.887192\n",
      "batch 1800: average train_loss=76.350502 \n",
      "training------1800, loss: 76.386658\n",
      "batch 1900: average train_loss=74.783951 \n",
      "training------1900, loss: 74.870369\n",
      "batch 2000: average train_loss=73.202911 \n",
      "training------2000, loss: 73.337587\n",
      "batch 2100: average train_loss=71.608093 \n",
      "training------2100, loss: 71.790576\n",
      "batch 2200: average train_loss=69.997604 \n",
      "training------2200, loss: 70.222881\n",
      "batch 2300: average train_loss=68.371948 \n",
      "training------2300, loss: 68.640388\n",
      "batch 2400: average train_loss=66.729889 \n",
      "training------2400, loss: 67.041323\n",
      "batch 2500: average train_loss=65.071228 \n",
      "training------2500, loss: 65.424797\n",
      "batch 2600: average train_loss=63.394051 \n",
      "training------2600, loss: 63.789316\n",
      "batch 2700: average train_loss=61.695385 \n",
      "training------2700, loss: 62.132538\n",
      "batch 2800: average train_loss=59.972679 \n",
      "training------2800, loss: 60.449107\n",
      "batch 2900: average train_loss=58.230068 \n",
      "training------2900, loss: 58.745529\n",
      "batch 3000: average train_loss=56.465443 \n",
      "training------3000, loss: 57.019341\n",
      "batch 3100: average train_loss=54.676491 \n",
      "training------3100, loss: 55.265948\n",
      "batch 3200: average train_loss=52.868130 \n",
      "training------3200, loss: 53.493641\n",
      "batch 3300: average train_loss=51.038506 \n",
      "training------3300, loss: 51.701749\n",
      "batch 3400: average train_loss=49.187584 \n",
      "training------3400, loss: 49.886411\n",
      "batch 3500: average train_loss=47.316582 \n",
      "training------3500, loss: 48.051179\n",
      "batch 3600: average train_loss=45.425499 \n",
      "training------3600, loss: 46.195373\n",
      "batch 3700: average train_loss=43.513752 \n",
      "training------3700, loss: 44.318402\n",
      "batch 3800: average train_loss=41.580196 \n",
      "training------3800, loss: 42.419642\n",
      "batch 3900: average train_loss=39.622314 \n",
      "training------3900, loss: 40.496083\n",
      "batch 4000: average train_loss=37.633987 \n",
      "training------4000, loss: 38.543343\n",
      "batch 4100: average train_loss=35.625835 \n",
      "training------4100, loss: 36.569558\n",
      "batch 4200: average train_loss=33.601185 \n",
      "training------4200, loss: 34.578172\n",
      "batch 4300: average train_loss=31.560265 \n",
      "training------4300, loss: 32.568780\n",
      "batch 4400: average train_loss=29.502016 \n",
      "training------4400, loss: 30.540543\n",
      "batch 4500: average train_loss=27.425993 \n",
      "training------4500, loss: 28.493620\n",
      "batch 4600: average train_loss=25.333399 \n",
      "training------4600, loss: 26.428347\n",
      "batch 4700: average train_loss=23.224863 \n",
      "training------4700, loss: 24.345481\n",
      "batch 4800: average train_loss=21.101919 \n",
      "training------4800, loss: 22.246408\n",
      "batch 4900: average train_loss=18.967348 \n",
      "training------4900, loss: 20.132898\n",
      "batch 5000: average train_loss=16.824636 \n",
      "training------5000, loss: 18.008405\n",
      "batch 5100: average train_loss=14.679870 \n",
      "training------5100, loss: 15.878530\n",
      "batch 5200: average train_loss=12.543636 \n",
      "training------5200, loss: 13.754893\n",
      "batch 5300: average train_loss=10.436304 \n",
      "training------5300, loss: 11.659916\n",
      "batch 5400: average train_loss=8.397731 \n",
      "training------5400, loss: 9.642778\n",
      "batch 5500: average train_loss=6.559382 \n",
      "training------5500, loss: 7.869834\n",
      "batch 5600: average train_loss=5.173130 \n",
      "training------5600, loss: 6.656842\n",
      "batch 5700: average train_loss=4.392688 \n",
      "training------5700, loss: 6.062960\n",
      "batch 5800: average train_loss=3.992259 \n",
      "training------5800, loss: 5.727916\n",
      "batch 5900: average train_loss=3.777125 \n",
      "training------5900, loss: 5.479618\n",
      "batch 6000: average train_loss=3.655388 \n",
      "training------6000, loss: 5.257103\n",
      "batch 6100: average train_loss=3.580759 \n",
      "training------6100, loss: 5.026689\n",
      "batch 6200: average train_loss=3.533558 \n",
      "training------6200, loss: 4.777698\n",
      "batch 6300: average train_loss=3.496139 \n",
      "training------6300, loss: 4.512628\n",
      "batch 6400: average train_loss=3.450269 \n",
      "training------6400, loss: 4.241427\n",
      "batch 6500: average train_loss=3.407473 \n",
      "training------6500, loss: 3.982556\n",
      "batch 6600: average train_loss=3.370912 \n",
      "training------6600, loss: 3.741213\n",
      "batch 6700: average train_loss=3.342432 \n",
      "training------6700, loss: 3.521019\n",
      "batch 6800: average train_loss=3.320557 \n",
      "training------6800, loss: 3.325936\n",
      "batch 6900: average train_loss=3.304871 \n",
      "training------6900, loss: 3.163205\n",
      "batch 7000: average train_loss=3.291775 \n",
      "training------7000, loss: 3.030475\n",
      "batch 7100: average train_loss=3.279721 \n",
      "training------7100, loss: 2.924497\n",
      "batch 7200: average train_loss=3.266962 \n",
      "training------7200, loss: 2.838174\n",
      "batch 7300: average train_loss=3.251633 \n",
      "training------7300, loss: 2.764974\n",
      "batch 7400: average train_loss=3.233447 \n",
      "training------7400, loss: 2.698375\n",
      "batch 7500: average train_loss=3.213943 \n",
      "training------7500, loss: 2.637204\n",
      "batch 7600: average train_loss=3.196697 \n",
      "training------7600, loss: 2.582338\n",
      "batch 7700: average train_loss=3.180911 \n",
      "training------7700, loss: 2.531694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 7800: average train_loss=3.165077 \n",
      "training------7800, loss: 2.483965\n",
      "batch 7900: average train_loss=3.147617 \n",
      "training------7900, loss: 2.439066\n",
      "batch 8000: average train_loss=3.130182 \n",
      "training------8000, loss: 2.395375\n",
      "batch 8100: average train_loss=3.110967 \n",
      "training------8100, loss: 2.353086\n",
      "batch 8200: average train_loss=3.091509 \n",
      "training------8200, loss: 2.313621\n",
      "batch 8300: average train_loss=3.070889 \n",
      "training------8300, loss: 2.276599\n",
      "batch 8400: average train_loss=3.051772 \n",
      "training------8400, loss: 2.241468\n",
      "batch 8500: average train_loss=3.030578 \n",
      "training------8500, loss: 2.210766\n",
      "batch 8600: average train_loss=3.013350 \n",
      "training------8600, loss: 2.183876\n",
      "batch 8700: average train_loss=2.997660 \n",
      "training------8700, loss: 2.155848\n",
      "batch 8800: average train_loss=2.982410 \n",
      "training------8800, loss: 2.130392\n",
      "batch 8900: average train_loss=2.969204 \n",
      "training------8900, loss: 2.106812\n",
      "batch 9000: average train_loss=2.954529 \n",
      "training------9000, loss: 2.082934\n",
      "batch 9100: average train_loss=2.941346 \n",
      "training------9100, loss: 2.063768\n",
      "batch 9200: average train_loss=2.928337 \n",
      "training------9200, loss: 2.044053\n",
      "batch 9300: average train_loss=2.915297 \n",
      "training------9300, loss: 2.026899\n",
      "batch 9400: average train_loss=2.901532 \n",
      "training------9400, loss: 2.008352\n",
      "batch 9500: average train_loss=2.888717 \n",
      "training------9500, loss: 1.989808\n",
      "batch 9600: average train_loss=2.877630 \n",
      "training------9600, loss: 1.975911\n",
      "batch 9700: average train_loss=2.866784 \n",
      "training------9700, loss: 1.958983\n",
      "batch 9800: average train_loss=2.856298 \n",
      "training------9800, loss: 1.948024\n",
      "batch 9900: average train_loss=2.846514 \n",
      "training------9900, loss: 1.933453\n",
      "batch 10000: average train_loss=2.836595 \n",
      "training------10000, loss: 1.919702\n",
      "batch 10100: average train_loss=2.827044 \n",
      "training------10100, loss: 1.906033\n",
      "batch 10200: average train_loss=2.817723 \n",
      "training------10200, loss: 1.893560\n",
      "batch 10300: average train_loss=2.809144 \n",
      "training------10300, loss: 1.880939\n",
      "batch 10400: average train_loss=2.800626 \n",
      "training------10400, loss: 1.871112\n",
      "batch 10500: average train_loss=2.793666 \n",
      "training------10500, loss: 1.860087\n",
      "batch 10600: average train_loss=2.786520 \n",
      "training------10600, loss: 1.845654\n",
      "batch 10700: average train_loss=2.779600 \n",
      "training------10700, loss: 1.837427\n",
      "batch 10800: average train_loss=2.772768 \n",
      "training------10800, loss: 1.826426\n",
      "batch 10900: average train_loss=2.765411 \n",
      "training------10900, loss: 1.814142\n",
      "batch 11000: average train_loss=2.758460 \n",
      "training------11000, loss: 1.802452\n",
      "batch 11100: average train_loss=2.750635 \n",
      "training------11100, loss: 1.792218\n",
      "batch 11200: average train_loss=2.742755 \n",
      "training------11200, loss: 1.781975\n",
      "batch 11300: average train_loss=2.735502 \n",
      "training------11300, loss: 1.771404\n",
      "batch 11400: average train_loss=2.727904 \n",
      "training------11400, loss: 1.761834\n",
      "batch 11500: average train_loss=2.722822 \n",
      "training------11500, loss: 1.746905\n",
      "batch 11600: average train_loss=2.716769 \n",
      "training------11600, loss: 1.737425\n",
      "batch 11700: average train_loss=2.711306 \n",
      "training------11700, loss: 1.727375\n",
      "batch 11800: average train_loss=2.704602 \n",
      "training------11800, loss: 1.717740\n",
      "batch 11900: average train_loss=2.694574 \n",
      "training------11900, loss: 1.713558\n",
      "batch 12000: average train_loss=2.688626 \n",
      "training------12000, loss: 1.702794\n",
      "batch 12100: average train_loss=2.682785 \n",
      "training------12100, loss: 1.694315\n",
      "batch 12200: average train_loss=2.677060 \n",
      "training------12200, loss: 1.687752\n",
      "batch 12300: average train_loss=2.671847 \n",
      "training------12300, loss: 1.679209\n",
      "batch 12400: average train_loss=2.666900 \n",
      "training------12400, loss: 1.670269\n",
      "batch 12500: average train_loss=2.662113 \n",
      "training------12500, loss: 1.662309\n",
      "batch 12600: average train_loss=2.658451 \n",
      "training------12600, loss: 1.647719\n",
      "batch 12700: average train_loss=2.655504 \n",
      "training------12700, loss: 1.638604\n",
      "batch 12800: average train_loss=2.651964 \n",
      "training------12800, loss: 1.630761\n",
      "batch 12900: average train_loss=2.646350 \n",
      "training------12900, loss: 1.622914\n",
      "batch 13000: average train_loss=2.641144 \n",
      "training------13000, loss: 1.614639\n",
      "batch 13100: average train_loss=2.635882 \n",
      "training------13100, loss: 1.605701\n",
      "batch 13200: average train_loss=2.627003 \n",
      "training------13200, loss: 1.604477\n",
      "batch 13300: average train_loss=2.625764 \n",
      "training------13300, loss: 1.590303\n",
      "batch 13400: average train_loss=2.617123 \n",
      "training------13400, loss: 1.590171\n",
      "batch 13500: average train_loss=2.611037 \n",
      "training------13500, loss: 1.583786\n",
      "batch 13600: average train_loss=2.605073 \n",
      "training------13600, loss: 1.576989\n",
      "batch 13700: average train_loss=2.599283 \n",
      "training------13700, loss: 1.569792\n",
      "batch 13800: average train_loss=2.593641 \n",
      "training------13800, loss: 1.561721\n",
      "batch 13900: average train_loss=2.589150 \n",
      "training------13900, loss: 1.546952\n",
      "batch 14000: average train_loss=2.583726 \n",
      "training------14000, loss: 1.547894\n",
      "batch 14100: average train_loss=2.583287 \n",
      "training------14100, loss: 1.530181\n",
      "batch 14200: average train_loss=2.580842 \n",
      "training------14200, loss: 1.523685\n",
      "batch 14300: average train_loss=2.569641 \n",
      "training------14300, loss: 1.525737\n",
      "batch 14400: average train_loss=2.569985 \n",
      "training------14400, loss: 1.509531\n",
      "batch 14500: average train_loss=2.558080 \n",
      "training------14500, loss: 1.511712\n",
      "batch 14600: average train_loss=2.552644 \n",
      "training------14600, loss: 1.504722\n",
      "batch 14700: average train_loss=2.548366 \n",
      "training------14700, loss: 1.496899\n",
      "batch 14800: average train_loss=2.550479 \n",
      "training------14800, loss: 1.478931\n",
      "batch 14900: average train_loss=2.542807 \n",
      "training------14900, loss: 1.474217\n",
      "batch 15000: average train_loss=2.533583 \n",
      "training------15000, loss: 1.474780\n",
      "batch 15100: average train_loss=2.533645 \n",
      "training------15100, loss: 1.458635\n",
      "batch 15200: average train_loss=2.523233 \n",
      "training------15200, loss: 1.460757\n",
      "batch 15300: average train_loss=2.522848 \n",
      "training------15300, loss: 1.442327\n",
      "batch 15400: average train_loss=2.521888 \n",
      "training------15400, loss: 1.435640\n",
      "batch 15500: average train_loss=2.517248 \n",
      "training------15500, loss: 1.429902\n",
      "batch 15600: average train_loss=2.505562 \n",
      "training------15600, loss: 1.431566\n",
      "batch 15700: average train_loss=2.500729 \n",
      "training------15700, loss: 1.425200\n",
      "batch 15800: average train_loss=2.496607 \n",
      "training------15800, loss: 1.415893\n",
      "batch 15900: average train_loss=2.501397 \n",
      "training------15900, loss: 1.401184\n",
      "batch 16000: average train_loss=2.497957 \n",
      "training------16000, loss: 1.395042\n",
      "batch 16100: average train_loss=2.488679 \n",
      "training------16100, loss: 1.394736\n",
      "batch 16200: average train_loss=2.482122 \n",
      "training------16200, loss: 1.390925\n",
      "batch 16300: average train_loss=2.478307 \n",
      "training------16300, loss: 1.384578\n",
      "batch 16400: average train_loss=2.480041 \n",
      "training------16400, loss: 1.368142\n",
      "batch 16500: average train_loss=2.481622 \n",
      "training------16500, loss: 1.361263\n",
      "batch 16600: average train_loss=2.477558 \n",
      "training------16600, loss: 1.355987\n",
      "batch 16700: average train_loss=2.464435 \n",
      "training------16700, loss: 1.356008\n",
      "batch 16800: average train_loss=2.461203 \n",
      "training------16800, loss: 1.349190\n",
      "batch 16900: average train_loss=2.468927 \n",
      "training------16900, loss: 1.332584\n",
      "batch 17000: average train_loss=2.465451 \n",
      "training------17000, loss: 1.324858\n",
      "batch 17100: average train_loss=2.459960 \n",
      "training------17100, loss: 1.320409\n",
      "batch 17200: average train_loss=2.459967 \n",
      "training------17200, loss: 1.311359\n",
      "batch 17300: average train_loss=2.454292 \n",
      "training------17300, loss: 1.306354\n",
      "batch 17400: average train_loss=2.453670 \n",
      "training------17400, loss: 1.297828\n",
      "batch 17500: average train_loss=2.450226 \n",
      "training------17500, loss: 1.291349\n",
      "batch 17600: average train_loss=2.446099 \n",
      "training------17600, loss: 1.285365\n",
      "batch 17700: average train_loss=2.443164 \n",
      "training------17700, loss: 1.277303\n",
      "batch 17800: average train_loss=2.441215 \n",
      "training------17800, loss: 1.273216\n",
      "batch 17900: average train_loss=2.439828 \n",
      "training------17900, loss: 1.265967\n",
      "batch 18000: average train_loss=2.435986 \n",
      "training------18000, loss: 1.258627\n",
      "batch 18100: average train_loss=2.431906 \n",
      "training------18100, loss: 1.254708\n",
      "batch 18200: average train_loss=2.430902 \n",
      "training------18200, loss: 1.245784\n",
      "batch 18300: average train_loss=2.427799 \n",
      "training------18300, loss: 1.240870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 18400: average train_loss=2.422379 \n",
      "training------18400, loss: 1.238240\n",
      "batch 18500: average train_loss=2.421263 \n",
      "training------18500, loss: 1.227898\n",
      "batch 18600: average train_loss=2.418209 \n",
      "training------18600, loss: 1.224596\n",
      "batch 18700: average train_loss=2.411538 \n",
      "training------18700, loss: 1.223617\n",
      "batch 18800: average train_loss=2.410273 \n",
      "training------18800, loss: 1.211421\n",
      "batch 18900: average train_loss=2.406972 \n",
      "training------18900, loss: 1.210161\n",
      "batch 19000: average train_loss=2.399698 \n",
      "training------19000, loss: 1.209356\n",
      "batch 19100: average train_loss=2.399162 \n",
      "training------19100, loss: 1.195587\n",
      "batch 19200: average train_loss=2.394667 \n",
      "training------19200, loss: 1.197147\n",
      "batch 19300: average train_loss=2.391176 \n",
      "training------19300, loss: 1.184005\n",
      "batch 19400: average train_loss=2.374245 \n",
      "training------19400, loss: 1.189851\n",
      "batch 19500: average train_loss=2.386295 \n",
      "training------19500, loss: 1.178587\n",
      "batch 19600: average train_loss=2.368801 \n",
      "training------19600, loss: 1.180732\n",
      "batch 19700: average train_loss=2.379834 \n",
      "training------19700, loss: 1.165833\n",
      "batch 19800: average train_loss=2.362475 \n",
      "training------19800, loss: 1.173059\n",
      "batch 19900: average train_loss=2.373444 \n",
      "training------19900, loss: 1.163317\n",
      "batch 20000: average train_loss=2.353117 \n",
      "training------20000, loss: 1.160111\n",
      "batch 20100: average train_loss=2.349340 \n",
      "training------20100, loss: 1.155886\n",
      "batch 20200: average train_loss=2.362773 \n",
      "training------20200, loss: 1.146717\n",
      "batch 20300: average train_loss=2.358848 \n",
      "training------20300, loss: 1.143909\n",
      "batch 20400: average train_loss=2.339467 \n",
      "training------20400, loss: 1.138829\n",
      "batch 20500: average train_loss=2.353009 \n",
      "training------20500, loss: 1.132182\n",
      "batch 20600: average train_loss=2.337125 \n",
      "training------20600, loss: 1.134627\n",
      "batch 20700: average train_loss=2.338667 \n",
      "training------20700, loss: 1.128567\n",
      "batch 20800: average train_loss=2.327875 \n",
      "training------20800, loss: 1.120577\n",
      "batch 20900: average train_loss=2.323999 \n",
      "training------20900, loss: 1.115322\n",
      "batch 21000: average train_loss=2.338999 \n",
      "training------21000, loss: 1.107676\n",
      "batch 21100: average train_loss=2.321380 \n",
      "training------21100, loss: 1.111582\n",
      "batch 21200: average train_loss=2.321678 \n",
      "training------21200, loss: 1.104935\n",
      "batch 21300: average train_loss=2.310392 \n",
      "training------21300, loss: 1.093371\n",
      "batch 21400: average train_loss=2.305612 \n",
      "training------21400, loss: 1.089955\n",
      "batch 21500: average train_loss=2.307301 \n",
      "training------21500, loss: 1.090825\n",
      "batch 21600: average train_loss=2.318826 \n",
      "training------21600, loss: 1.085363\n",
      "batch 21700: average train_loss=2.297198 \n",
      "training------21700, loss: 1.074948\n",
      "batch 21800: average train_loss=2.294451 \n",
      "training------21800, loss: 1.078549\n",
      "batch 21900: average train_loss=2.305409 \n",
      "training------21900, loss: 1.070968\n",
      "batch 22000: average train_loss=2.307258 \n",
      "training------22000, loss: 1.064032\n",
      "batch 22100: average train_loss=2.299813 \n",
      "training------22100, loss: 1.054853\n",
      "batch 22200: average train_loss=2.281308 \n",
      "training------22200, loss: 1.057031\n",
      "batch 22300: average train_loss=2.277761 \n",
      "training------22300, loss: 1.057997\n",
      "batch 22400: average train_loss=2.282492 \n",
      "training------22400, loss: 1.055116\n",
      "batch 22500: average train_loss=2.290293 \n",
      "training------22500, loss: 1.046477\n",
      "batch 22600: average train_loss=2.291096 \n",
      "training------22600, loss: 1.039636\n",
      "batch 22700: average train_loss=2.274494 \n",
      "training------22700, loss: 1.034755\n",
      "batch 22800: average train_loss=2.264047 \n",
      "training------22800, loss: 1.036898\n",
      "batch 22900: average train_loss=2.261701 \n",
      "training------22900, loss: 1.038487\n",
      "batch 23000: average train_loss=2.267460 \n",
      "training------23000, loss: 1.034606\n",
      "batch 23100: average train_loss=2.275535 \n",
      "training------23100, loss: 1.026337\n",
      "batch 23200: average train_loss=2.277067 \n",
      "training------23200, loss: 1.020837\n",
      "batch 23300: average train_loss=2.266849 \n",
      "training------23300, loss: 1.014170\n",
      "batch 23400: average train_loss=2.251458 \n",
      "training------23400, loss: 1.017669\n",
      "batch 23500: average train_loss=2.248100 \n",
      "training------23500, loss: 1.018692\n",
      "batch 23600: average train_loss=2.249272 \n",
      "training------23600, loss: 1.017854\n",
      "batch 23700: average train_loss=2.259460 \n",
      "training------23700, loss: 1.009540\n",
      "batch 23800: average train_loss=2.263202 \n",
      "training------23800, loss: 1.004049\n",
      "batch 23900: average train_loss=2.264160 \n",
      "training------23900, loss: 0.997959\n",
      "batch 24000: average train_loss=2.247597 \n",
      "training------24000, loss: 0.995481\n",
      "batch 24100: average train_loss=2.237183 \n",
      "training------24100, loss: 0.998351\n",
      "batch 24200: average train_loss=2.234087 \n",
      "training------24200, loss: 0.999302\n",
      "batch 24300: average train_loss=2.234841 \n",
      "training------24300, loss: 0.999526\n",
      "batch 24400: average train_loss=2.244191 \n",
      "training------24400, loss: 0.992007\n",
      "batch 24500: average train_loss=2.249575 \n",
      "training------24500, loss: 0.985990\n",
      "batch 24600: average train_loss=2.251168 \n",
      "training------24600, loss: 0.981366\n",
      "batch 24700: average train_loss=2.242597 \n",
      "training------24700, loss: 0.974015\n",
      "batch 24800: average train_loss=2.226325 \n",
      "training------24800, loss: 0.978358\n",
      "batch 24900: average train_loss=2.220896 \n",
      "training------24900, loss: 0.980011\n",
      "batch 25000: average train_loss=2.218344 \n",
      "training------25000, loss: 0.981164\n",
      "batch 25100: average train_loss=2.222323 \n",
      "training------25100, loss: 0.977491\n",
      "batch 25200: average train_loss=2.230912 \n",
      "training------25200, loss: 0.970775\n",
      "batch 25300: average train_loss=2.234244 \n",
      "training------25300, loss: 0.965778\n",
      "batch 25400: average train_loss=2.235068 \n",
      "training------25400, loss: 0.961378\n",
      "batch 25500: average train_loss=2.226747 \n",
      "training------25500, loss: 0.954397\n",
      "batch 25600: average train_loss=2.210991 \n",
      "training------25600, loss: 0.958231\n",
      "batch 25700: average train_loss=2.204810 \n",
      "training------25700, loss: 0.959970\n",
      "batch 25800: average train_loss=2.202256 \n",
      "training------25800, loss: 0.961139\n",
      "batch 25900: average train_loss=2.202972 \n",
      "training------25900, loss: 0.960477\n",
      "batch 26000: average train_loss=2.211396 \n",
      "training------26000, loss: 0.954185\n",
      "batch 26100: average train_loss=2.218916 \n",
      "training------26100, loss: 0.948493\n",
      "batch 26200: average train_loss=2.221364 \n",
      "training------26200, loss: 0.944656\n",
      "batch 26300: average train_loss=2.222057 \n",
      "training------26300, loss: 0.938012\n",
      "batch 26400: average train_loss=2.205037 \n",
      "training------26400, loss: 0.936604\n",
      "batch 26500: average train_loss=2.194741 \n",
      "training------26500, loss: 0.940904\n",
      "batch 26600: average train_loss=2.191727 \n",
      "training------26600, loss: 0.942169\n",
      "batch 26700: average train_loss=2.191474 \n",
      "training------26700, loss: 0.943905\n",
      "batch 26800: average train_loss=2.199271 \n",
      "training------26800, loss: 0.938168\n",
      "batch 26900: average train_loss=2.208629 \n",
      "training------26900, loss: 0.931110\n",
      "batch 27000: average train_loss=2.211855 \n",
      "training------27000, loss: 0.927330\n",
      "batch 27100: average train_loss=2.213279 \n",
      "training------27100, loss: 0.920168\n",
      "batch 27200: average train_loss=2.195392 \n",
      "training------27200, loss: 0.919555\n",
      "batch 27300: average train_loss=2.184229 \n",
      "training------27300, loss: 0.924058\n",
      "batch 27400: average train_loss=2.180827 \n",
      "training------27400, loss: 0.924887\n",
      "batch 27500: average train_loss=2.181998 \n",
      "training------27500, loss: 0.925835\n",
      "batch 27600: average train_loss=2.194083 \n",
      "training------27600, loss: 0.918102\n",
      "batch 27700: average train_loss=2.200849 \n",
      "training------27700, loss: 0.912590\n",
      "batch 27800: average train_loss=2.203465 \n",
      "training------27800, loss: 0.907320\n",
      "batch 27900: average train_loss=2.185229 \n",
      "training------27900, loss: 0.902877\n",
      "batch 28000: average train_loss=2.172888 \n",
      "training------28000, loss: 0.908836\n",
      "batch 28100: average train_loss=2.170514 \n",
      "training------28100, loss: 0.911098\n",
      "batch 28200: average train_loss=2.176006 \n",
      "training------28200, loss: 0.909399\n",
      "batch 28300: average train_loss=2.188349 \n",
      "training------28300, loss: 0.901706\n",
      "batch 28400: average train_loss=2.192786 \n",
      "training------28400, loss: 0.896720\n",
      "batch 28500: average train_loss=2.192046 \n",
      "training------28500, loss: 0.886318\n",
      "batch 28600: average train_loss=2.168056 \n",
      "training------28600, loss: 0.892199\n",
      "batch 28700: average train_loss=2.161848 \n",
      "training------28700, loss: 0.896084\n",
      "batch 28800: average train_loss=2.161487 \n",
      "training------28800, loss: 0.899406\n",
      "batch 28900: average train_loss=2.174078 \n",
      "training------28900, loss: 0.891452\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 29000: average train_loss=2.182802 \n",
      "training------29000, loss: 0.885162\n",
      "batch 29100: average train_loss=2.185569 \n",
      "training------29100, loss: 0.879033\n",
      "batch 29200: average train_loss=2.164017 \n",
      "training------29200, loss: 0.875511\n",
      "batch 29300: average train_loss=2.152086 \n",
      "training------29300, loss: 0.882562\n",
      "batch 29400: average train_loss=2.149974 \n",
      "training------29400, loss: 0.885976\n",
      "batch 29500: average train_loss=2.157465 \n",
      "training------29500, loss: 0.881660\n",
      "batch 29600: average train_loss=2.169200 \n",
      "training------29600, loss: 0.874048\n",
      "batch 29700: average train_loss=2.172885 \n",
      "training------29700, loss: 0.870103\n",
      "batch 29800: average train_loss=2.168137 \n",
      "training------29800, loss: 0.858957\n",
      "batch 29900: average train_loss=2.144844 \n",
      "training------29900, loss: 0.867074\n",
      "batch 30000: average train_loss=2.139875 \n",
      "training------30000, loss: 0.870805\n",
      "batch 30100: average train_loss=2.140290 \n",
      "training------30100, loss: 0.873881\n",
      "batch 30200: average train_loss=2.154109 \n",
      "training------30200, loss: 0.864834\n",
      "batch 30300: average train_loss=2.162962 \n",
      "training------30300, loss: 0.859298\n",
      "batch 30400: average train_loss=2.166093 \n",
      "training------30400, loss: 0.851527\n",
      "batch 30500: average train_loss=2.139613 \n",
      "training------30500, loss: 0.853357\n",
      "batch 30600: average train_loss=2.131810 \n",
      "training------30600, loss: 0.858813\n",
      "batch 30700: average train_loss=2.131605 \n",
      "training------30700, loss: 0.862585\n",
      "batch 30800: average train_loss=2.145600 \n",
      "training------30800, loss: 0.853459\n",
      "batch 30900: average train_loss=2.154987 \n",
      "training------30900, loss: 0.847933\n",
      "batch 31000: average train_loss=2.158223 \n",
      "training------31000, loss: 0.841840\n",
      "batch 31100: average train_loss=2.133492 \n",
      "training------31100, loss: 0.840142\n",
      "batch 31200: average train_loss=2.123930 \n",
      "training------31200, loss: 0.845809\n",
      "batch 31300: average train_loss=2.122683 \n",
      "training------31300, loss: 0.849892\n",
      "batch 31400: average train_loss=2.134864 \n",
      "training------31400, loss: 0.843544\n",
      "batch 31500: average train_loss=2.145692 \n",
      "training------31500, loss: 0.837706\n",
      "batch 31600: average train_loss=2.149261 \n",
      "training------31600, loss: 0.833448\n",
      "batch 31700: average train_loss=2.130010 \n",
      "training------31700, loss: 0.825338\n",
      "batch 31800: average train_loss=2.115750 \n",
      "training------31800, loss: 0.833551\n",
      "batch 31900: average train_loss=2.113307 \n",
      "training------31900, loss: 0.837768\n",
      "batch 32000: average train_loss=2.123204 \n",
      "training------32000, loss: 0.833415\n",
      "batch 32100: average train_loss=2.136395 \n",
      "training------32100, loss: 0.826856\n",
      "batch 32200: average train_loss=2.141132 \n",
      "training------32200, loss: 0.822999\n",
      "batch 32300: average train_loss=2.121356 \n",
      "training------32300, loss: 0.814377\n",
      "batch 32400: average train_loss=2.107743 \n",
      "training------32400, loss: 0.823435\n",
      "batch 32500: average train_loss=2.105943 \n",
      "training------32500, loss: 0.828144\n",
      "batch 32600: average train_loss=2.121047 \n",
      "training------32600, loss: 0.820954\n",
      "batch 32700: average train_loss=2.132041 \n",
      "training------32700, loss: 0.815600\n",
      "batch 32800: average train_loss=2.135204 \n",
      "training------32800, loss: 0.805724\n",
      "batch 32900: average train_loss=2.104890 \n",
      "training------32900, loss: 0.810334\n",
      "batch 33000: average train_loss=2.099133 \n",
      "training------33000, loss: 0.815589\n",
      "batch 33100: average train_loss=2.105848 \n",
      "training------33100, loss: 0.814457\n",
      "batch 33200: average train_loss=2.122901 \n",
      "training------33200, loss: 0.807427\n",
      "batch 33300: average train_loss=2.128351 \n",
      "training------33300, loss: 0.803899\n",
      "batch 33400: average train_loss=2.107233 \n",
      "training------33400, loss: 0.795386\n",
      "batch 33500: average train_loss=2.093921 \n",
      "training------33500, loss: 0.804372\n",
      "batch 33600: average train_loss=2.093751 \n",
      "training------33600, loss: 0.809413\n",
      "batch 33700: average train_loss=2.113292 \n",
      "training------33700, loss: 0.800324\n",
      "batch 33800: average train_loss=2.121513 \n",
      "training------33800, loss: 0.796432\n",
      "batch 33900: average train_loss=2.112320 \n",
      "training------33900, loss: 0.782810\n",
      "batch 34000: average train_loss=2.090079 \n",
      "training------34000, loss: 0.794234\n",
      "batch 34100: average train_loss=2.088057 \n",
      "training------34100, loss: 0.799396\n",
      "batch 34200: average train_loss=2.106036 \n",
      "training------34200, loss: 0.792803\n",
      "batch 34300: average train_loss=2.117019 \n",
      "training------34300, loss: 0.788534\n",
      "batch 34400: average train_loss=2.109341 \n",
      "training------34400, loss: 0.774565\n",
      "batch 34500: average train_loss=2.085719 \n",
      "training------34500, loss: 0.786869\n",
      "batch 34600: average train_loss=2.083468 \n",
      "training------34600, loss: 0.791821\n",
      "batch 34700: average train_loss=2.105497 \n",
      "training------34700, loss: 0.783024\n",
      "batch 34800: average train_loss=2.113822 \n",
      "training------34800, loss: 0.779981\n",
      "batch 34900: average train_loss=2.091318 \n",
      "training------34900, loss: 0.772187\n",
      "batch 35000: average train_loss=2.079923 \n",
      "training------35000, loss: 0.780698\n",
      "batch 35100: average train_loss=2.091766 \n",
      "training------35100, loss: 0.778466\n",
      "batch 35200: average train_loss=2.108588 \n",
      "training------35200, loss: 0.774000\n",
      "batch 35300: average train_loss=2.102302 \n",
      "training------35300, loss: 0.759033\n",
      "batch 35400: average train_loss=2.077722 \n",
      "training------35400, loss: 0.771662\n",
      "batch 35500: average train_loss=2.078460 \n",
      "training------35500, loss: 0.775722\n",
      "batch 35600: average train_loss=2.102861 \n",
      "training------35600, loss: 0.768388\n",
      "batch 35700: average train_loss=2.109780 \n",
      "training------35700, loss: 0.763339\n",
      "batch 35800: average train_loss=2.077602 \n",
      "training------35800, loss: 0.762046\n",
      "batch 35900: average train_loss=2.072097 \n",
      "training------35900, loss: 0.769227\n",
      "batch 36000: average train_loss=2.093679 \n",
      "training------36000, loss: 0.763800\n",
      "batch 36100: average train_loss=2.104513 \n",
      "training------36100, loss: 0.760672\n",
      "batch 36200: average train_loss=2.079826 \n",
      "training------36200, loss: 0.752294\n",
      "batch 36300: average train_loss=2.069146 \n",
      "training------36300, loss: 0.761600\n",
      "batch 36400: average train_loss=2.087462 \n",
      "training------36400, loss: 0.758579\n",
      "batch 36500: average train_loss=2.101147 \n",
      "training------36500, loss: 0.754611\n",
      "batch 36600: average train_loss=2.076499 \n",
      "training------36600, loss: 0.745985\n",
      "batch 36700: average train_loss=2.065439 \n",
      "training------36700, loss: 0.756545\n",
      "batch 36800: average train_loss=2.086861 \n",
      "training------36800, loss: 0.752176\n",
      "batch 36900: average train_loss=2.099021 \n",
      "training------36900, loss: 0.748192\n",
      "batch 37000: average train_loss=2.066650 \n",
      "training------37000, loss: 0.744308\n",
      "batch 37100: average train_loss=2.061404 \n",
      "training------37100, loss: 0.754885\n",
      "batch 37200: average train_loss=2.089599 \n",
      "training------37200, loss: 0.744563\n",
      "batch 37300: average train_loss=2.093902 \n",
      "training------37300, loss: 0.730816\n",
      "batch 37400: average train_loss=2.058928 \n",
      "training------37400, loss: 0.741807\n",
      "batch 37500: average train_loss=2.067645 \n",
      "training------37500, loss: 0.741356\n",
      "batch 37600: average train_loss=2.089263 \n",
      "training------37600, loss: 0.738295\n",
      "batch 37700: average train_loss=2.071376 \n",
      "training------37700, loss: 0.725896\n",
      "batch 37800: average train_loss=2.053650 \n",
      "training------37800, loss: 0.739559\n",
      "batch 37900: average train_loss=2.074383 \n",
      "training------37900, loss: 0.735698\n",
      "batch 38000: average train_loss=2.088536 \n",
      "training------38000, loss: 0.732403\n",
      "batch 38100: average train_loss=2.054615 \n",
      "training------38100, loss: 0.729519\n",
      "batch 38200: average train_loss=2.050086 \n",
      "training------38200, loss: 0.739911\n",
      "batch 38300: average train_loss=2.079619 \n",
      "training------38300, loss: 0.728557\n",
      "batch 38400: average train_loss=2.066234 \n",
      "training------38400, loss: 0.713662\n",
      "batch 38500: average train_loss=2.044845 \n",
      "training------38500, loss: 0.730142\n",
      "batch 38600: average train_loss=2.068071 \n",
      "training------38600, loss: 0.726491\n",
      "batch 38700: average train_loss=2.081435 \n",
      "training------38700, loss: 0.713797\n",
      "batch 38800: average train_loss=2.041753 \n",
      "training------38800, loss: 0.724476\n",
      "batch 38900: average train_loss=2.059025 \n",
      "training------38900, loss: 0.723812\n",
      "batch 39000: average train_loss=2.077802 \n",
      "training------39000, loss: 0.720289\n",
      "batch 39100: average train_loss=2.039662 \n",
      "training------39100, loss: 0.719092\n",
      "batch 39200: average train_loss=2.049426 \n",
      "training------39200, loss: 0.718380\n",
      "batch 39300: average train_loss=2.070692 \n",
      "training------39300, loss: 0.717272\n",
      "batch 39400: average train_loss=2.043220 \n",
      "training------39400, loss: 0.710476\n",
      "batch 39500: average train_loss=2.032809 \n",
      "training------39500, loss: 0.724914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 39600: average train_loss=2.064039 \n",
      "training------39600, loss: 0.713798\n",
      "batch 39700: average train_loss=2.049320 \n",
      "training------39700, loss: 0.699299\n",
      "batch 39800: average train_loss=2.027988 \n",
      "training------39800, loss: 0.715144\n",
      "batch 39900: average train_loss=2.051872 \n",
      "training------39900, loss: 0.712345\n",
      "batch 40000: average train_loss=2.067393 \n",
      "training------40000, loss: 0.704547\n",
      "batch 40100: average train_loss=2.025568 \n",
      "training------40100, loss: 0.708262\n",
      "batch 40200: average train_loss=2.040461 \n",
      "training------40200, loss: 0.709539\n",
      "batch 40300: average train_loss=2.059618 \n",
      "training------40300, loss: 0.706821\n",
      "batch 40400: average train_loss=2.028332 \n",
      "training------40400, loss: 0.700522\n",
      "batch 40500: average train_loss=2.021930 \n",
      "training------40500, loss: 0.713205\n",
      "batch 40600: average train_loss=2.053517 \n",
      "training------40600, loss: 0.703462\n",
      "batch 40700: average train_loss=2.038890 \n",
      "training------40700, loss: 0.687779\n",
      "batch 40800: average train_loss=2.017596 \n",
      "training------40800, loss: 0.703394\n",
      "batch 40900: average train_loss=2.040989 \n",
      "training------40900, loss: 0.702094\n",
      "batch 41000: average train_loss=2.056781 \n",
      "training------41000, loss: 0.695994\n",
      "batch 41100: average train_loss=2.015363 \n",
      "training------41100, loss: 0.695553\n",
      "batch 41200: average train_loss=2.028807 \n",
      "training------41200, loss: 0.697576\n",
      "batch 41300: average train_loss=2.049251 \n",
      "training------41300, loss: 0.696060\n",
      "batch 41400: average train_loss=2.017025 \n",
      "training------41400, loss: 0.688502\n",
      "batch 41500: average train_loss=2.014892 \n",
      "training------41500, loss: 0.695115\n",
      "batch 41600: average train_loss=2.043973 \n",
      "training------41600, loss: 0.692766\n",
      "batch 41700: average train_loss=2.019445 \n",
      "training------41700, loss: 0.680954\n",
      "batch 41800: average train_loss=2.005889 \n",
      "training------41800, loss: 0.697822\n",
      "batch 41900: average train_loss=2.038280 \n",
      "training------41900, loss: 0.688893\n",
      "batch 42000: average train_loss=2.021601 \n",
      "training------42000, loss: 0.673618\n",
      "batch 42100: average train_loss=2.001108 \n",
      "training------42100, loss: 0.690724\n",
      "batch 42200: average train_loss=2.032156 \n",
      "training------42200, loss: 0.686034\n",
      "batch 42300: average train_loss=2.024047 \n",
      "training------42300, loss: 0.667319\n",
      "batch 42400: average train_loss=1.996944 \n",
      "training------42400, loss: 0.683530\n",
      "batch 42500: average train_loss=2.021117 \n",
      "training------42500, loss: 0.684369\n",
      "batch 42600: average train_loss=2.037079 \n",
      "training------42600, loss: 0.671125\n",
      "batch 42700: average train_loss=1.993062 \n",
      "training------42700, loss: 0.678912\n",
      "batch 42800: average train_loss=2.012979 \n",
      "training------42800, loss: 0.681597\n",
      "batch 42900: average train_loss=2.033029 \n",
      "training------42900, loss: 0.675052\n",
      "batch 43000: average train_loss=1.988693 \n",
      "training------43000, loss: 0.676090\n",
      "batch 43100: average train_loss=2.016286 \n",
      "training------43100, loss: 0.677537\n",
      "batch 43200: average train_loss=2.008868 \n",
      "training------43200, loss: 0.657140\n",
      "batch 43300: average train_loss=1.984739 \n",
      "training------43300, loss: 0.678001\n",
      "batch 43400: average train_loss=2.018879 \n",
      "training------43400, loss: 0.672863\n",
      "batch 43500: average train_loss=1.998812 \n",
      "training------43500, loss: 0.657963\n",
      "batch 43600: average train_loss=1.982050 \n",
      "training------43600, loss: 0.679336\n",
      "batch 43700: average train_loss=2.016518 \n",
      "training------43700, loss: 0.669610\n",
      "batch 43800: average train_loss=1.989143 \n",
      "training------43800, loss: 0.659546\n",
      "batch 43900: average train_loss=1.990490 \n",
      "training------43900, loss: 0.667836\n",
      "batch 44000: average train_loss=2.014751 \n",
      "training------44000, loss: 0.666299\n",
      "batch 44100: average train_loss=1.976069 \n",
      "training------44100, loss: 0.662069\n",
      "batch 44200: average train_loss=1.992819 \n",
      "training------44200, loss: 0.667384\n",
      "batch 44300: average train_loss=1.998281 \n",
      "training------44300, loss: 0.643571\n",
      "batch 44400: average train_loss=1.970391 \n",
      "training------44400, loss: 0.665812\n",
      "batch 44500: average train_loss=2.006060 \n",
      "training------44500, loss: 0.660995\n",
      "batch 44600: average train_loss=1.972119 \n",
      "training------44600, loss: 0.653871\n",
      "batch 44700: average train_loss=1.985710 \n",
      "training------44700, loss: 0.662433\n",
      "batch 44800: average train_loss=1.996326 \n",
      "training------44800, loss: 0.636941\n",
      "batch 44900: average train_loss=1.965001 \n",
      "training------44900, loss: 0.660197\n",
      "batch 45000: average train_loss=2.002015 \n",
      "training------45000, loss: 0.656527\n",
      "batch 45100: average train_loss=1.964935 \n",
      "training------45100, loss: 0.650123\n",
      "batch 45200: average train_loss=1.985082 \n",
      "training------45200, loss: 0.658516\n",
      "batch 45300: average train_loss=1.987312 \n",
      "training------45300, loss: 0.632550\n",
      "batch 45400: average train_loss=2.002467 \n",
      "training------45400, loss: 0.658577\n",
      "batch 45500: average train_loss=2.000040 \n",
      "training------45500, loss: 0.646087\n",
      "batch 45600: average train_loss=1.982680 \n",
      "training------45600, loss: 0.654283\n",
      "batch 45700: average train_loss=1.998214 \n",
      "training------45700, loss: 0.643079\n",
      "batch 45800: average train_loss=1.983463 \n",
      "training------45800, loss: 0.652231\n",
      "batch 45900: average train_loss=1.994010 \n",
      "training------45900, loss: 0.649102\n",
      "batch 46000: average train_loss=1.994881 \n",
      "training------46000, loss: 0.638776\n",
      "batch 46100: average train_loss=1.982305 \n",
      "training------46100, loss: 0.649043\n",
      "batch 46200: average train_loss=1.993237 \n",
      "training------46200, loss: 0.635669\n",
      "batch 46300: average train_loss=1.976249 \n",
      "training------46300, loss: 0.648974\n",
      "batch 46400: average train_loss=1.991562 \n",
      "training------46400, loss: 0.641687\n",
      "batch 46500: average train_loss=1.985329 \n",
      "training------46500, loss: 0.640245\n",
      "batch 46600: average train_loss=1.980822 \n",
      "training------46600, loss: 0.644957\n",
      "batch 46700: average train_loss=1.988500 \n",
      "training------46700, loss: 0.630206\n",
      "batch 46800: average train_loss=1.971338 \n",
      "training------46800, loss: 0.644361\n",
      "batch 46900: average train_loss=1.986580 \n",
      "training------46900, loss: 0.636197\n",
      "batch 47000: average train_loss=1.977295 \n",
      "training------47000, loss: 0.638035\n",
      "batch 47100: average train_loss=1.976359 \n",
      "training------47100, loss: 0.640721\n",
      "batch 47200: average train_loss=1.983275 \n",
      "training------47200, loss: 0.625524\n",
      "batch 47300: average train_loss=1.966124 \n",
      "training------47300, loss: 0.639674\n",
      "batch 47400: average train_loss=1.981742 \n",
      "training------47400, loss: 0.629472\n",
      "batch 47500: average train_loss=1.967147 \n",
      "training------47500, loss: 0.636387\n",
      "batch 47600: average train_loss=1.975706 \n",
      "training------47600, loss: 0.635230\n",
      "batch 47700: average train_loss=1.978107 \n",
      "training------47700, loss: 0.623713\n",
      "batch 47800: average train_loss=1.964937 \n",
      "training------47800, loss: 0.634510\n",
      "batch 47900: average train_loss=1.976789 \n",
      "training------47900, loss: 0.619772\n",
      "batch 48000: average train_loss=1.957992 \n",
      "training------48000, loss: 0.632890\n",
      "batch 48100: average train_loss=1.974714 \n",
      "training------48100, loss: 0.622702\n",
      "batch 48200: average train_loss=1.960093 \n",
      "training------48200, loss: 0.629316\n",
      "batch 48300: average train_loss=1.969604 \n",
      "training------48300, loss: 0.628856\n",
      "batch 48400: average train_loss=1.970875 \n",
      "training------48400, loss: 0.617914\n",
      "batch 48500: average train_loss=1.956861 \n",
      "training------48500, loss: 0.627554\n",
      "batch 48600: average train_loss=1.971020 \n",
      "training------48600, loss: 0.613985\n",
      "batch 48700: average train_loss=1.950950 \n",
      "training------48700, loss: 0.626867\n",
      "batch 48800: average train_loss=1.969165 \n",
      "training------48800, loss: 0.616759\n",
      "batch 48900: average train_loss=1.955072 \n",
      "training------48900, loss: 0.622555\n",
      "batch 49000: average train_loss=1.962314 \n",
      "training------49000, loss: 0.623677\n",
      "batch 49100: average train_loss=1.964533 \n",
      "training------49100, loss: 0.611428\n",
      "batch 49200: average train_loss=1.949437 \n",
      "training------49200, loss: 0.621345\n",
      "batch 49300: average train_loss=1.964541 \n",
      "training------49300, loss: 0.607880\n",
      "batch 49400: average train_loss=1.943700 \n",
      "training------49400, loss: 0.620584\n",
      "batch 49500: average train_loss=1.962875 \n",
      "training------49500, loss: 0.611390\n",
      "batch 49600: average train_loss=1.951234 \n",
      "training------49600, loss: 0.613668\n",
      "batch 49700: average train_loss=1.953146 \n",
      "training------49700, loss: 0.617327\n",
      "batch 49800: average train_loss=1.959090 \n",
      "training------49800, loss: 0.603501\n",
      "batch 49900: average train_loss=1.939821 \n",
      "training------49900, loss: 0.615072\n",
      "training------49999, loss: 0.610662\n",
      "End...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import util.LAMSGeneration as LAMSFunction\n",
    "import tensorflow.compat.v1 as tf\n",
    "from util.args import argument_parser, model_kwargs, train_kwargs\n",
    "from util.models import CNNModel\n",
    "from util.omniglot import read_dataset\n",
    "from util.train import train\n",
    "from datetime import datetime\n",
    "\n",
    "# the floor plan image\n",
    "imageArry = np.array(cv2.imread(\"../imagedata/41-124gray.jpg\", cv2.IMREAD_GRAYSCALE))\n",
    "# measurement data and id\n",
    "infoExcel = \"data/MeasureData.xlsx\"\n",
    "fileData = pd.read_excel(infoExcel,sheet_name=\"data\", header=None)\n",
    "fileID = pd.read_excel(infoExcel,sheet_name=\"ID\", header=None)\n",
    "saveDir = \"data/Beam\"\n",
    "lams = LAMSFunction.LAMS(imageArry,fileData,fileID,saveDir)\n",
    "lams.findVertices()\n",
    "\n",
    "print(\"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now()))\n",
    "datetime_ = \"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now())\n",
    "args = argument_parser(datetime_).parse_args(args=[])\n",
    "\n",
    "train_set=[]\n",
    "for i in range(1,16): \n",
    "    train_set = train_set+read_dataset(saveDir+str(i)+\"/\") #\n",
    "\n",
    "model = CNNModel(**model_kwargs(args))\n",
    "with tf.Session() as sess:\n",
    "    if not args.pretrained:\n",
    "        print('Training...')\n",
    "        train(sess, model, train_set, args.checkpoint, **train_kwargs(args))\n",
    "    else:\n",
    "        print('Restoring from checkpoint...')\n",
    "        tf.train.Saver().restore(sess, tf.train.latest_checkpoint(args.checkpoint))\n",
    "\n",
    "    print('End...')\n",
    "    print(\"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9f2c4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-27-23-15-49/\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:22: conv2d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/keras/legacy_tf_layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:23: batch_normalization (from tensorflow.python.keras.legacy_tf_layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:35: flatten (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Flatten instead.\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:40: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/cine/mmwave/indoor_Pei/Training_Model/util/models.py:43: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "Training...\n",
      "INFO:tensorflow:Summary name average loss is illegal; using average_loss instead.\n",
      "batch 0: average train_loss=105.839661 \n",
      "training------0, loss: 105.537839\n",
      "batch 100: average train_loss=103.260178 \n",
      "training------100, loss: 102.861589\n",
      "batch 200: average train_loss=100.927765 \n",
      "training------200, loss: 100.484290\n",
      "batch 300: average train_loss=98.911552 \n",
      "training------300, loss: 98.498343\n",
      "batch 400: average train_loss=97.075989 \n",
      "training------400, loss: 96.711631\n",
      "batch 500: average train_loss=95.363174 \n",
      "training------500, loss: 95.047493\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "batch 600: average train_loss=93.710693 \n",
      "training------600, loss: 93.443436\n",
      "batch 700: average train_loss=92.075813 \n",
      "training------700, loss: 91.847967\n",
      "batch 800: average train_loss=90.457390 \n",
      "training------800, loss: 90.257065\n",
      "batch 900: average train_loss=88.852463 \n",
      "training------900, loss: 88.684457\n",
      "batch 1000: average train_loss=87.266068 \n",
      "training------1000, loss: 87.140902\n",
      "batch 1100: average train_loss=85.700325 \n",
      "training------1100, loss: 85.622687\n",
      "batch 1200: average train_loss=84.145744 \n",
      "training------1200, loss: 84.118303\n",
      "batch 1300: average train_loss=82.594246 \n",
      "training------1300, loss: 82.612412\n",
      "batch 1400: average train_loss=81.044144 \n",
      "training------1400, loss: 81.107457\n",
      "batch 1500: average train_loss=79.493790 \n",
      "training------1500, loss: 79.604736\n",
      "batch 1600: average train_loss=77.947525 \n",
      "training------1600, loss: 78.105230\n",
      "batch 1700: average train_loss=76.395615 \n",
      "training------1700, loss: 76.599842\n",
      "batch 1800: average train_loss=74.833344 \n",
      "training------1800, loss: 75.084559\n",
      "batch 1900: average train_loss=73.255234 \n",
      "training------1900, loss: 73.553073\n",
      "batch 2000: average train_loss=71.652580 \n",
      "training------2000, loss: 71.994011\n",
      "batch 2100: average train_loss=70.025719 \n",
      "training------2100, loss: 70.403260\n",
      "batch 2200: average train_loss=68.385223 \n",
      "training------2200, loss: 68.801331\n",
      "batch 2300: average train_loss=66.725601 \n",
      "training------2300, loss: 67.181905\n",
      "batch 2400: average train_loss=65.046387 \n",
      "training------2400, loss: 65.542633\n",
      "batch 2500: average train_loss=63.353935 \n",
      "training------2500, loss: 63.889370\n",
      "batch 2600: average train_loss=61.645519 \n",
      "training------2600, loss: 62.220428\n",
      "batch 2700: average train_loss=59.920444 \n",
      "training------2700, loss: 60.535117\n",
      "batch 2800: average train_loss=58.177082 \n",
      "training------2800, loss: 58.832121\n",
      "batch 2900: average train_loss=56.413483 \n",
      "training------2900, loss: 57.108932\n",
      "batch 3000: average train_loss=54.629326 \n",
      "training------3000, loss: 55.362498\n",
      "batch 3100: average train_loss=52.821980 \n",
      "training------3100, loss: 53.593879\n",
      "batch 3200: average train_loss=50.993492 \n",
      "training------3200, loss: 51.804034\n",
      "batch 3300: average train_loss=49.141804 \n",
      "training------3300, loss: 49.992154\n",
      "batch 3400: average train_loss=47.268124 \n",
      "training------3400, loss: 48.158689\n",
      "batch 3500: average train_loss=45.371525 \n",
      "training------3500, loss: 46.302187\n",
      "batch 3600: average train_loss=43.452244 \n",
      "training------3600, loss: 44.423599\n",
      "batch 3700: average train_loss=41.512123 \n",
      "training------3700, loss: 42.523630\n",
      "batch 3800: average train_loss=39.551022 \n",
      "training------3800, loss: 40.602027\n",
      "batch 3900: average train_loss=37.569702 \n",
      "training------3900, loss: 38.659635\n",
      "batch 4000: average train_loss=35.567799 \n",
      "training------4000, loss: 36.696252\n",
      "batch 4100: average train_loss=33.545418 \n",
      "training------4100, loss: 34.711410\n",
      "batch 4200: average train_loss=31.503721 \n",
      "training------4200, loss: 32.706267\n",
      "batch 4300: average train_loss=29.441957 \n",
      "training------4300, loss: 30.679492\n",
      "batch 4400: average train_loss=27.357632 \n",
      "training------4400, loss: 28.630021\n",
      "batch 4500: average train_loss=25.251413 \n",
      "training------4500, loss: 26.556889\n",
      "batch 4600: average train_loss=23.125084 \n",
      "training------4600, loss: 24.463074\n",
      "batch 4700: average train_loss=20.984005 \n",
      "training------4700, loss: 22.354792\n",
      "batch 4800: average train_loss=18.828773 \n",
      "training------4800, loss: 20.232029\n",
      "batch 4900: average train_loss=16.662409 \n",
      "training------4900, loss: 18.097992\n",
      "batch 5000: average train_loss=14.492393 \n",
      "training------5000, loss: 15.959563\n",
      "batch 5100: average train_loss=12.310746 \n",
      "training------5100, loss: 13.800155\n",
      "batch 5200: average train_loss=10.166097 \n",
      "training------5200, loss: 11.690940\n",
      "batch 5300: average train_loss=8.121569 \n",
      "training------5300, loss: 9.726117\n",
      "batch 5400: average train_loss=6.322728 \n",
      "training------5400, loss: 8.102273\n",
      "batch 5500: average train_loss=5.031811 \n",
      "training------5500, loss: 7.083275\n",
      "batch 5600: average train_loss=4.289072 \n",
      "training------5600, loss: 6.531585\n",
      "batch 5700: average train_loss=3.899319 \n",
      "training------5700, loss: 6.170489\n",
      "batch 5800: average train_loss=3.704330 \n",
      "training------5800, loss: 5.882862\n",
      "batch 5900: average train_loss=3.618513 \n",
      "training------5900, loss: 5.612030\n",
      "batch 6000: average train_loss=3.584250 \n",
      "training------6000, loss: 5.330782\n",
      "batch 6100: average train_loss=3.564474 \n",
      "training------6100, loss: 5.039592\n",
      "batch 6200: average train_loss=3.542238 \n",
      "training------6200, loss: 4.752735\n",
      "batch 6300: average train_loss=3.516622 \n",
      "training------6300, loss: 4.474481\n",
      "batch 6400: average train_loss=3.491321 \n",
      "training------6400, loss: 4.213029\n",
      "batch 6500: average train_loss=3.471018 \n",
      "training------6500, loss: 3.971446\n",
      "batch 6600: average train_loss=3.458068 \n",
      "training------6600, loss: 3.754452\n",
      "batch 6700: average train_loss=3.450873 \n",
      "training------6700, loss: 3.563986\n",
      "batch 6800: average train_loss=3.444074 \n",
      "training------6800, loss: 3.401018\n",
      "batch 6900: average train_loss=3.437155 \n",
      "training------6900, loss: 3.264793\n",
      "batch 7500: average train_loss=3.352992 \n",
      "training------7500, loss: 2.796108\n",
      "batch 7600: average train_loss=3.331457 \n",
      "training------7600, loss: 2.744440\n",
      "batch 7700: average train_loss=3.308877 \n",
      "training------7700, loss: 2.698292\n",
      "batch 7800: average train_loss=3.286982 \n",
      "training------7800, loss: 2.655699\n",
      "batch 7900: average train_loss=3.265014 \n",
      "training------7900, loss: 2.615491\n",
      "batch 8000: average train_loss=3.242436 \n",
      "training------8000, loss: 2.576573\n",
      "batch 8100: average train_loss=3.219487 \n",
      "training------8100, loss: 2.539284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 8200: average train_loss=3.196558 \n",
      "training------8200, loss: 2.503730\n",
      "batch 8300: average train_loss=3.173949 \n",
      "training------8300, loss: 2.469093\n",
      "batch 8400: average train_loss=3.154511 \n",
      "training------8400, loss: 2.435443\n",
      "batch 8500: average train_loss=3.136985 \n",
      "training------8500, loss: 2.403034\n",
      "batch 8600: average train_loss=3.121123 \n",
      "training------8600, loss: 2.371448\n",
      "batch 8700: average train_loss=3.105016 \n",
      "training------8700, loss: 2.340465\n",
      "batch 8800: average train_loss=3.089265 \n",
      "training------8800, loss: 2.309554\n",
      "batch 8900: average train_loss=3.073067 \n",
      "training------8900, loss: 2.279017\n",
      "batch 9000: average train_loss=3.056256 \n",
      "training------9000, loss: 2.248018\n",
      "batch 9100: average train_loss=3.040288 \n",
      "training------9100, loss: 2.217779\n",
      "batch 9200: average train_loss=3.025315 \n",
      "training------9200, loss: 2.187982\n",
      "batch 9300: average train_loss=3.012120 \n",
      "training------9300, loss: 2.158674\n",
      "batch 9400: average train_loss=2.998021 \n",
      "training------9400, loss: 2.128747\n",
      "batch 9500: average train_loss=2.984106 \n",
      "training------9500, loss: 2.098754\n",
      "batch 9600: average train_loss=2.970290 \n",
      "training------9600, loss: 2.069331\n",
      "batch 9700: average train_loss=2.954602 \n",
      "training------9700, loss: 2.041116\n",
      "batch 9800: average train_loss=2.937671 \n",
      "training------9800, loss: 2.012879\n",
      "batch 9900: average train_loss=2.920349 \n",
      "training------9900, loss: 1.985950\n",
      "batch 10000: average train_loss=2.903705 \n",
      "training------10000, loss: 1.961429\n",
      "batch 10100: average train_loss=2.888561 \n",
      "training------10100, loss: 1.938639\n",
      "batch 10200: average train_loss=2.872967 \n",
      "training------10200, loss: 1.916345\n",
      "batch 10300: average train_loss=2.856910 \n",
      "training------10300, loss: 1.894744\n",
      "batch 10400: average train_loss=2.841461 \n",
      "training------10400, loss: 1.874284\n",
      "batch 10500: average train_loss=2.827393 \n",
      "training------10500, loss: 1.854983\n",
      "batch 10600: average train_loss=2.814617 \n",
      "training------10600, loss: 1.837400\n",
      "batch 10700: average train_loss=2.802274 \n",
      "training------10700, loss: 1.820649\n",
      "batch 10800: average train_loss=2.790361 \n",
      "training------10800, loss: 1.804710\n",
      "batch 10900: average train_loss=2.779139 \n",
      "training------10900, loss: 1.789311\n",
      "batch 11000: average train_loss=2.767951 \n",
      "training------11000, loss: 1.774336\n",
      "batch 11100: average train_loss=2.756800 \n",
      "training------11100, loss: 1.760403\n",
      "batch 11200: average train_loss=2.746256 \n",
      "training------11200, loss: 1.747673\n",
      "batch 11300: average train_loss=2.735914 \n",
      "training------11300, loss: 1.735301\n",
      "batch 11400: average train_loss=2.726910 \n",
      "training------11400, loss: 1.722860\n",
      "batch 11500: average train_loss=2.718410 \n",
      "training------11500, loss: 1.711026\n",
      "batch 11600: average train_loss=2.710125 \n",
      "training------11600, loss: 1.699776\n",
      "batch 11700: average train_loss=2.701317 \n",
      "training------11700, loss: 1.688589\n",
      "batch 11800: average train_loss=2.692464 \n",
      "training------11800, loss: 1.677567\n",
      "batch 11900: average train_loss=2.683528 \n",
      "training------11900, loss: 1.666833\n",
      "batch 12000: average train_loss=2.676707 \n",
      "training------12000, loss: 1.656217\n",
      "batch 12100: average train_loss=2.669257 \n",
      "training------12100, loss: 1.645780\n",
      "batch 12200: average train_loss=2.663396 \n",
      "training------12200, loss: 1.635055\n",
      "batch 12300: average train_loss=2.656178 \n",
      "training------12300, loss: 1.624570\n",
      "batch 12400: average train_loss=2.649235 \n",
      "training------12400, loss: 1.614214\n",
      "batch 12500: average train_loss=2.642605 \n",
      "training------12500, loss: 1.603897\n",
      "batch 12600: average train_loss=2.636074 \n",
      "training------12600, loss: 1.593822\n",
      "batch 12700: average train_loss=2.629498 \n",
      "training------12700, loss: 1.584171\n",
      "batch 12800: average train_loss=2.622813 \n",
      "training------12800, loss: 1.574442\n",
      "batch 12900: average train_loss=2.616169 \n",
      "training------12900, loss: 1.564558\n",
      "batch 13000: average train_loss=2.609008 \n",
      "training------13000, loss: 1.554874\n",
      "batch 13100: average train_loss=2.601919 \n",
      "training------13100, loss: 1.545292\n",
      "batch 13200: average train_loss=2.594496 \n",
      "training------13200, loss: 1.535883\n",
      "batch 13300: average train_loss=2.587823 \n",
      "training------13300, loss: 1.526510\n",
      "batch 13400: average train_loss=2.582705 \n",
      "training------13400, loss: 1.516925\n",
      "batch 13500: average train_loss=2.576766 \n",
      "training------13500, loss: 1.507306\n",
      "batch 13600: average train_loss=2.570113 \n",
      "training------13600, loss: 1.497957\n",
      "batch 13700: average train_loss=2.563129 \n",
      "training------13700, loss: 1.488357\n",
      "batch 14200: average train_loss=2.531928 \n",
      "training------14200, loss: 1.442118\n",
      "batch 14300: average train_loss=2.526434 \n",
      "training------14300, loss: 1.432805\n",
      "batch 14400: average train_loss=2.521022 \n",
      "training------14400, loss: 1.424157\n",
      "batch 14500: average train_loss=2.514444 \n",
      "training------14500, loss: 1.415485\n",
      "batch 14600: average train_loss=2.507592 \n",
      "training------14600, loss: 1.406611\n",
      "batch 14700: average train_loss=2.500999 \n",
      "training------14700, loss: 1.397789\n",
      "batch 14800: average train_loss=2.495045 \n",
      "training------14800, loss: 1.389336\n",
      "batch 14900: average train_loss=2.489015 \n",
      "training------14900, loss: 1.380897\n",
      "batch 15000: average train_loss=2.482407 \n",
      "training------15000, loss: 1.372658\n",
      "batch 15100: average train_loss=2.475080 \n",
      "training------15100, loss: 1.364384\n",
      "batch 15200: average train_loss=2.467766 \n",
      "training------15200, loss: 1.355943\n",
      "batch 15300: average train_loss=2.460790 \n",
      "training------15300, loss: 1.347457\n",
      "batch 15400: average train_loss=2.454054 \n",
      "training------15400, loss: 1.339239\n",
      "batch 15500: average train_loss=2.448041 \n",
      "training------15500, loss: 1.331332\n",
      "batch 15600: average train_loss=2.442224 \n",
      "training------15600, loss: 1.323746\n",
      "batch 15700: average train_loss=2.435213 \n",
      "training------15700, loss: 1.316648\n",
      "batch 15800: average train_loss=2.428335 \n",
      "training------15800, loss: 1.309311\n",
      "batch 15900: average train_loss=2.422348 \n",
      "training------15900, loss: 1.301885\n",
      "batch 16000: average train_loss=2.414992 \n",
      "training------16000, loss: 1.294772\n",
      "batch 16100: average train_loss=2.410446 \n",
      "training------16100, loss: 1.286861\n",
      "batch 16200: average train_loss=2.403269 \n",
      "training------16200, loss: 1.280748\n",
      "batch 16300: average train_loss=2.398092 \n",
      "training------16300, loss: 1.273491\n",
      "batch 16400: average train_loss=2.393843 \n",
      "training------16400, loss: 1.266520\n",
      "batch 16500: average train_loss=2.387175 \n",
      "training------16500, loss: 1.260519\n",
      "batch 16600: average train_loss=2.383652 \n",
      "training------16600, loss: 1.252650\n",
      "batch 16700: average train_loss=2.376991 \n",
      "training------16700, loss: 1.247670\n",
      "batch 16800: average train_loss=2.372457 \n",
      "training------16800, loss: 1.239851\n",
      "batch 16900: average train_loss=2.365731 \n",
      "training------16900, loss: 1.234560\n",
      "batch 17400: average train_loss=2.338280 \n",
      "training------17400, loss: 1.201678\n",
      "batch 17500: average train_loss=2.332498 \n",
      "training------17500, loss: 1.194392\n",
      "batch 17600: average train_loss=2.325971 \n",
      "training------17600, loss: 1.189679\n",
      "batch 17700: average train_loss=2.321594 \n",
      "training------17700, loss: 1.183538\n",
      "batch 17800: average train_loss=2.316430 \n",
      "training------17800, loss: 1.175431\n",
      "batch 17900: average train_loss=2.309464 \n",
      "training------17900, loss: 1.170650\n",
      "batch 18000: average train_loss=2.303016 \n",
      "training------18000, loss: 1.165805\n",
      "batch 18100: average train_loss=2.297271 \n",
      "training------18100, loss: 1.159611\n",
      "batch 18200: average train_loss=2.291565 \n",
      "training------18200, loss: 1.153834\n",
      "batch 18300: average train_loss=2.286120 \n",
      "training------18300, loss: 1.148298\n",
      "batch 18400: average train_loss=2.281048 \n",
      "training------18400, loss: 1.142724\n",
      "batch 18500: average train_loss=2.275643 \n",
      "training------18500, loss: 1.137029\n",
      "batch 18600: average train_loss=2.269691 \n",
      "training------18600, loss: 1.131735\n",
      "batch 18700: average train_loss=2.263331 \n",
      "training------18700, loss: 1.126799\n",
      "batch 18800: average train_loss=2.257109 \n",
      "training------18800, loss: 1.121628\n",
      "batch 18900: average train_loss=2.250447 \n",
      "training------18900, loss: 1.116980\n",
      "batch 19000: average train_loss=2.243678 \n",
      "training------19000, loss: 1.112332\n",
      "batch 19100: average train_loss=2.237161 \n",
      "training------19100, loss: 1.107325\n",
      "batch 19200: average train_loss=2.231012 \n",
      "training------19200, loss: 1.102443\n",
      "batch 19300: average train_loss=2.225241 \n",
      "training------19300, loss: 1.098502\n",
      "batch 19400: average train_loss=2.219580 \n",
      "training------19400, loss: 1.094397\n",
      "batch 19500: average train_loss=2.214642 \n",
      "training------19500, loss: 1.089563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 19600: average train_loss=2.210733 \n",
      "training------19600, loss: 1.082841\n",
      "batch 19700: average train_loss=2.206121 \n",
      "training------19700, loss: 1.078431\n",
      "batch 19800: average train_loss=2.200858 \n",
      "training------19800, loss: 1.075092\n",
      "batch 19900: average train_loss=2.195121 \n",
      "training------19900, loss: 1.071899\n",
      "batch 20000: average train_loss=2.188833 \n",
      "training------20000, loss: 1.067794\n",
      "batch 20100: average train_loss=2.182981 \n",
      "training------20100, loss: 1.063544\n",
      "batch 20200: average train_loss=2.177667 \n",
      "training------20200, loss: 1.060021\n",
      "batch 20300: average train_loss=2.173110 \n",
      "training------20300, loss: 1.056010\n",
      "batch 20400: average train_loss=2.169019 \n",
      "training------20400, loss: 1.050064\n",
      "batch 20500: average train_loss=2.164652 \n",
      "training------20500, loss: 1.044728\n",
      "batch 20600: average train_loss=2.159670 \n",
      "training------20600, loss: 1.040834\n",
      "batch 20700: average train_loss=2.154365 \n",
      "training------20700, loss: 1.037658\n",
      "batch 20800: average train_loss=2.148466 \n",
      "training------20800, loss: 1.034556\n",
      "batch 20900: average train_loss=2.142196 \n",
      "training------20900, loss: 1.030850\n",
      "batch 21000: average train_loss=2.136564 \n",
      "training------21000, loss: 1.027072\n",
      "batch 21100: average train_loss=2.131612 \n",
      "training------21100, loss: 1.024042\n",
      "batch 21200: average train_loss=2.127783 \n",
      "training------21200, loss: 1.019278\n",
      "batch 21300: average train_loss=2.124263 \n",
      "training------21300, loss: 1.013475\n",
      "batch 21400: average train_loss=2.119985 \n",
      "training------21400, loss: 1.010595\n",
      "batch 21500: average train_loss=2.114605 \n",
      "training------21500, loss: 1.008430\n",
      "batch 21600: average train_loss=2.108467 \n",
      "training------21600, loss: 1.005290\n",
      "batch 21700: average train_loss=2.102463 \n",
      "training------21700, loss: 1.001276\n",
      "batch 21800: average train_loss=2.097425 \n",
      "training------21800, loss: 0.998576\n",
      "batch 21900: average train_loss=2.093450 \n",
      "training------21900, loss: 0.995302\n",
      "batch 22000: average train_loss=2.090717 \n",
      "training------22000, loss: 0.989383\n",
      "batch 22100: average train_loss=2.086956 \n",
      "training------22100, loss: 0.985488\n",
      "batch 22200: average train_loss=2.082862 \n",
      "training------22200, loss: 0.983008\n",
      "batch 22300: average train_loss=2.078275 \n",
      "training------22300, loss: 0.980322\n",
      "batch 22400: average train_loss=2.072957 \n",
      "training------22400, loss: 0.976977\n",
      "batch 22500: average train_loss=2.067342 \n",
      "training------22500, loss: 0.972976\n",
      "batch 22600: average train_loss=2.062340 \n",
      "training------22600, loss: 0.969874\n",
      "batch 22700: average train_loss=2.058172 \n",
      "training------22700, loss: 0.966516\n",
      "batch 22800: average train_loss=2.054861 \n",
      "training------22800, loss: 0.962109\n",
      "batch 22900: average train_loss=2.052255 \n",
      "training------22900, loss: 0.956576\n",
      "batch 23000: average train_loss=2.049168 \n",
      "training------23000, loss: 0.952659\n",
      "batch 23100: average train_loss=2.045550 \n",
      "training------23100, loss: 0.949945\n",
      "batch 23200: average train_loss=2.041708 \n",
      "training------23200, loss: 0.947916\n",
      "batch 23300: average train_loss=2.037386 \n",
      "training------23300, loss: 0.945823\n",
      "batch 23400: average train_loss=2.032824 \n",
      "training------23400, loss: 0.942667\n",
      "batch 23500: average train_loss=2.028009 \n",
      "training------23500, loss: 0.939287\n",
      "batch 23600: average train_loss=2.023659 \n",
      "training------23600, loss: 0.936382\n",
      "batch 23700: average train_loss=2.019524 \n",
      "training------23700, loss: 0.933995\n",
      "batch 23800: average train_loss=2.015724 \n",
      "training------23800, loss: 0.931654\n",
      "batch 23900: average train_loss=2.012636 \n",
      "training------23900, loss: 0.929086\n",
      "batch 24000: average train_loss=2.009926 \n",
      "training------24000, loss: 0.926057\n",
      "batch 24100: average train_loss=2.007625 \n",
      "training------24100, loss: 0.922127\n",
      "batch 24200: average train_loss=2.005247 \n",
      "training------24200, loss: 0.918370\n",
      "batch 24300: average train_loss=2.002589 \n",
      "training------24300, loss: 0.914903\n",
      "batch 24400: average train_loss=1.999820 \n",
      "training------24400, loss: 0.911827\n",
      "batch 24500: average train_loss=1.996897 \n",
      "training------24500, loss: 0.909067\n",
      "batch 24600: average train_loss=1.994132 \n",
      "training------24600, loss: 0.906400\n",
      "batch 24700: average train_loss=1.991059 \n",
      "training------24700, loss: 0.903761\n",
      "batch 24800: average train_loss=1.987979 \n",
      "training------24800, loss: 0.900928\n",
      "batch 24900: average train_loss=1.984765 \n",
      "training------24900, loss: 0.898256\n",
      "batch 25000: average train_loss=1.981637 \n",
      "training------25000, loss: 0.895778\n",
      "batch 25100: average train_loss=1.978499 \n",
      "training------25100, loss: 0.893789\n",
      "batch 25200: average train_loss=1.975626 \n",
      "training------25200, loss: 0.891918\n",
      "batch 25300: average train_loss=1.972648 \n",
      "training------25300, loss: 0.889947\n",
      "batch 25400: average train_loss=1.969313 \n",
      "training------25400, loss: 0.887986\n",
      "batch 25500: average train_loss=1.965916 \n",
      "training------25500, loss: 0.886147\n",
      "batch 25600: average train_loss=1.962590 \n",
      "training------25600, loss: 0.883769\n",
      "batch 25700: average train_loss=1.959206 \n",
      "training------25700, loss: 0.880985\n",
      "batch 25800: average train_loss=1.955741 \n",
      "training------25800, loss: 0.878190\n",
      "batch 25900: average train_loss=1.952550 \n",
      "training------25900, loss: 0.875548\n",
      "batch 26000: average train_loss=1.949509 \n",
      "training------26000, loss: 0.872490\n",
      "batch 26100: average train_loss=1.946557 \n",
      "training------26100, loss: 0.869604\n",
      "batch 26200: average train_loss=1.943853 \n",
      "training------26200, loss: 0.867145\n",
      "batch 26300: average train_loss=1.941797 \n",
      "training------26300, loss: 0.865127\n",
      "batch 26400: average train_loss=1.940098 \n",
      "training------26400, loss: 0.863340\n",
      "batch 26500: average train_loss=1.938344 \n",
      "training------26500, loss: 0.861439\n",
      "batch 26600: average train_loss=1.936181 \n",
      "training------26600, loss: 0.858900\n",
      "batch 26700: average train_loss=1.933614 \n",
      "training------26700, loss: 0.855885\n",
      "batch 26800: average train_loss=1.930558 \n",
      "training------26800, loss: 0.852888\n",
      "batch 27300: average train_loss=1.913976 \n",
      "training------27300, loss: 0.842350\n",
      "batch 27400: average train_loss=1.913544 \n",
      "training------27400, loss: 0.841418\n",
      "batch 27500: average train_loss=1.911512 \n",
      "training------27500, loss: 0.838649\n",
      "batch 27600: average train_loss=1.908439 \n",
      "training------27600, loss: 0.835743\n",
      "batch 27700: average train_loss=1.904293 \n",
      "training------27700, loss: 0.834505\n",
      "batch 27800: average train_loss=1.899695 \n",
      "training------27800, loss: 0.833353\n",
      "batch 27900: average train_loss=1.895984 \n",
      "training------27900, loss: 0.830165\n",
      "batch 28000: average train_loss=1.893262 \n",
      "training------28000, loss: 0.827746\n",
      "batch 28100: average train_loss=1.891645 \n",
      "training------28100, loss: 0.826330\n",
      "batch 28200: average train_loss=1.890332 \n",
      "training------28200, loss: 0.825067\n",
      "batch 28300: average train_loss=1.889087 \n",
      "training------28300, loss: 0.823549\n",
      "batch 28400: average train_loss=1.887304 \n",
      "training------28400, loss: 0.821445\n",
      "batch 28500: average train_loss=1.884883 \n",
      "training------28500, loss: 0.818686\n",
      "batch 28600: average train_loss=1.882229 \n",
      "training------28600, loss: 0.815949\n",
      "batch 28700: average train_loss=1.879316 \n",
      "training------28700, loss: 0.813550\n",
      "batch 28800: average train_loss=1.876299 \n",
      "training------28800, loss: 0.811460\n",
      "batch 28900: average train_loss=1.872834 \n",
      "training------28900, loss: 0.810115\n",
      "batch 29000: average train_loss=1.869382 \n",
      "training------29000, loss: 0.808586\n",
      "batch 29100: average train_loss=1.865890 \n",
      "training------29100, loss: 0.806628\n",
      "batch 29200: average train_loss=1.862714 \n",
      "training------29200, loss: 0.804430\n",
      "batch 29300: average train_loss=1.859603 \n",
      "training------29300, loss: 0.801917\n",
      "batch 29400: average train_loss=1.856536 \n",
      "training------29400, loss: 0.798909\n",
      "batch 29500: average train_loss=1.853630 \n",
      "training------29500, loss: 0.795840\n",
      "batch 29600: average train_loss=1.851182 \n",
      "training------29600, loss: 0.793029\n",
      "batch 29700: average train_loss=1.849212 \n",
      "training------29700, loss: 0.790306\n",
      "batch 29800: average train_loss=1.848229 \n",
      "training------29800, loss: 0.788386\n",
      "batch 29900: average train_loss=1.847952 \n",
      "training------29900, loss: 0.786934\n",
      "batch 30000: average train_loss=1.846637 \n",
      "training------30000, loss: 0.784396\n",
      "batch 30100: average train_loss=1.844253 \n",
      "training------30100, loss: 0.781361\n",
      "batch 30200: average train_loss=1.840796 \n",
      "training------30200, loss: 0.778582\n",
      "batch 30300: average train_loss=1.835516 \n",
      "training------30300, loss: 0.777257\n",
      "batch 30400: average train_loss=1.830915 \n",
      "training------30400, loss: 0.774190\n",
      "batch 30500: average train_loss=1.828710 \n",
      "training------30500, loss: 0.771285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30600: average train_loss=1.829397 \n",
      "training------30600, loss: 0.770835\n",
      "batch 30700: average train_loss=1.828958 \n",
      "training------30700, loss: 0.769137\n",
      "batch 30800: average train_loss=1.826554 \n",
      "training------30800, loss: 0.766163\n",
      "batch 30900: average train_loss=1.823043 \n",
      "training------30900, loss: 0.763734\n",
      "batch 31000: average train_loss=1.817967 \n",
      "training------31000, loss: 0.762635\n",
      "batch 31100: average train_loss=1.813029 \n",
      "training------31100, loss: 0.760480\n",
      "batch 31200: average train_loss=1.810117 \n",
      "training------31200, loss: 0.757757\n",
      "batch 31300: average train_loss=1.808391 \n",
      "training------31300, loss: 0.755266\n",
      "batch 31400: average train_loss=1.808300 \n",
      "training------31400, loss: 0.753951\n",
      "batch 31500: average train_loss=1.808586 \n",
      "training------31500, loss: 0.753104\n",
      "batch 31600: average train_loss=1.806943 \n",
      "training------31600, loss: 0.750576\n",
      "batch 31700: average train_loss=1.804475 \n",
      "training------31700, loss: 0.747666\n",
      "batch 31800: average train_loss=1.800632 \n",
      "training------31800, loss: 0.746129\n",
      "batch 31900: average train_loss=1.795390 \n",
      "training------31900, loss: 0.745386\n",
      "batch 32000: average train_loss=1.791467 \n",
      "training------32000, loss: 0.742786\n",
      "batch 32100: average train_loss=1.789500 \n",
      "training------32100, loss: 0.740587\n",
      "batch 32200: average train_loss=1.788713 \n",
      "training------32200, loss: 0.738941\n",
      "batch 32300: average train_loss=1.788775 \n",
      "training------32300, loss: 0.738273\n",
      "batch 32400: average train_loss=1.788473 \n",
      "training------32400, loss: 0.736845\n",
      "batch 32500: average train_loss=1.787117 \n",
      "training------32500, loss: 0.734520\n",
      "batch 32600: average train_loss=1.784701 \n",
      "training------32600, loss: 0.732102\n",
      "batch 32700: average train_loss=1.782036 \n",
      "training------32700, loss: 0.729991\n",
      "batch 32800: average train_loss=1.778887 \n",
      "training------32800, loss: 0.728507\n",
      "batch 32900: average train_loss=1.774600 \n",
      "training------32900, loss: 0.727372\n",
      "batch 33000: average train_loss=1.771171 \n",
      "training------33000, loss: 0.724989\n",
      "batch 33100: average train_loss=1.769252 \n",
      "training------33100, loss: 0.722504\n",
      "batch 33200: average train_loss=1.768019 \n",
      "training------33200, loss: 0.720522\n",
      "batch 33300: average train_loss=1.767119 \n",
      "training------33300, loss: 0.718798\n",
      "batch 33400: average train_loss=1.766814 \n",
      "training------33400, loss: 0.717770\n",
      "batch 33500: average train_loss=1.766846 \n",
      "training------33500, loss: 0.716881\n",
      "batch 33600: average train_loss=1.766617 \n",
      "training------33600, loss: 0.715279\n",
      "batch 33700: average train_loss=1.765621 \n",
      "training------33700, loss: 0.712770\n",
      "batch 33800: average train_loss=1.764303 \n",
      "training------33800, loss: 0.709716\n",
      "batch 33900: average train_loss=1.762593 \n",
      "training------33900, loss: 0.707314\n",
      "batch 34000: average train_loss=1.760396 \n",
      "training------34000, loss: 0.705550\n",
      "batch 34100: average train_loss=1.757093 \n",
      "training------34100, loss: 0.704282\n",
      "batch 34200: average train_loss=1.754004 \n",
      "training------34200, loss: 0.702162\n",
      "batch 34300: average train_loss=1.751505 \n",
      "training------34300, loss: 0.699684\n",
      "batch 34400: average train_loss=1.749811 \n",
      "training------34400, loss: 0.697121\n",
      "batch 34500: average train_loss=1.749151 \n",
      "training------34500, loss: 0.695015\n",
      "batch 34600: average train_loss=1.748968 \n",
      "training------34600, loss: 0.693757\n",
      "batch 34700: average train_loss=1.748944 \n",
      "training------34700, loss: 0.692709\n",
      "batch 34800: average train_loss=1.748233 \n",
      "training------34800, loss: 0.691494\n",
      "batch 34900: average train_loss=1.746890 \n",
      "training------34900, loss: 0.688907\n",
      "batch 35000: average train_loss=1.744846 \n",
      "training------35000, loss: 0.686471\n",
      "batch 35100: average train_loss=1.742427 \n",
      "training------35100, loss: 0.684593\n",
      "batch 35200: average train_loss=1.739639 \n",
      "training------35200, loss: 0.683185\n",
      "batch 35300: average train_loss=1.736007 \n",
      "training------35300, loss: 0.681863\n",
      "batch 35400: average train_loss=1.732652 \n",
      "training------35400, loss: 0.679956\n",
      "batch 35500: average train_loss=1.730374 \n",
      "training------35500, loss: 0.677595\n",
      "batch 35600: average train_loss=1.728690 \n",
      "training------35600, loss: 0.675516\n",
      "batch 35700: average train_loss=1.728253 \n",
      "training------35700, loss: 0.674126\n",
      "batch 35800: average train_loss=1.728707 \n",
      "training------35800, loss: 0.673279\n",
      "batch 35900: average train_loss=1.728619 \n",
      "training------35900, loss: 0.672042\n",
      "batch 36000: average train_loss=1.727569 \n",
      "training------36000, loss: 0.670362\n",
      "batch 36100: average train_loss=1.726201 \n",
      "training------36100, loss: 0.667997\n",
      "batch 36200: average train_loss=1.724568 \n",
      "training------36200, loss: 0.665732\n",
      "batch 36300: average train_loss=1.722558 \n",
      "training------36300, loss: 0.663683\n",
      "batch 36400: average train_loss=1.719687 \n",
      "training------36400, loss: 0.662642\n",
      "batch 36500: average train_loss=1.716352 \n",
      "training------36500, loss: 0.661330\n",
      "batch 36600: average train_loss=1.713333 \n",
      "training------36600, loss: 0.659287\n",
      "batch 36700: average train_loss=1.710899 \n",
      "training------36700, loss: 0.657255\n",
      "batch 36800: average train_loss=1.709386 \n",
      "training------36800, loss: 0.655371\n",
      "batch 36900: average train_loss=1.708568 \n",
      "training------36900, loss: 0.653796\n",
      "batch 37000: average train_loss=1.709157 \n",
      "training------37000, loss: 0.652920\n",
      "batch 37100: average train_loss=1.709649 \n",
      "training------37100, loss: 0.652687\n",
      "batch 37200: average train_loss=1.709795 \n",
      "training------37200, loss: 0.651661\n",
      "batch 37300: average train_loss=1.709592 \n",
      "training------37300, loss: 0.650306\n",
      "batch 37400: average train_loss=1.708775 \n",
      "training------37400, loss: 0.648341\n",
      "batch 37500: average train_loss=1.707698 \n",
      "training------37500, loss: 0.645896\n",
      "batch 37600: average train_loss=1.706558 \n",
      "training------37600, loss: 0.643760\n",
      "batch 37700: average train_loss=1.705198 \n",
      "training------37700, loss: 0.641908\n",
      "batch 37800: average train_loss=1.703642 \n",
      "training------37800, loss: 0.640688\n",
      "batch 37900: average train_loss=1.701684 \n",
      "training------37900, loss: 0.639872\n",
      "batch 38000: average train_loss=1.699342 \n",
      "training------38000, loss: 0.639067\n",
      "batch 38100: average train_loss=1.697202 \n",
      "training------38100, loss: 0.637788\n",
      "batch 38200: average train_loss=1.695177 \n",
      "training------38200, loss: 0.636277\n",
      "batch 38300: average train_loss=1.693431 \n",
      "training------38300, loss: 0.634459\n",
      "batch 38400: average train_loss=1.691968 \n",
      "training------38400, loss: 0.632974\n",
      "batch 38500: average train_loss=1.690590 \n",
      "training------38500, loss: 0.631371\n",
      "batch 38600: average train_loss=1.689455 \n",
      "training------38600, loss: 0.629783\n",
      "batch 38700: average train_loss=1.688480 \n",
      "training------38700, loss: 0.628275\n",
      "batch 38800: average train_loss=1.687630 \n",
      "training------38800, loss: 0.626886\n",
      "batch 38900: average train_loss=1.687084 \n",
      "training------38900, loss: 0.625618\n",
      "batch 39000: average train_loss=1.686707 \n",
      "training------39000, loss: 0.624304\n",
      "batch 39100: average train_loss=1.686823 \n",
      "training------39100, loss: 0.623379\n",
      "batch 39200: average train_loss=1.687704 \n",
      "training------39200, loss: 0.622933\n",
      "batch 39300: average train_loss=1.688289 \n",
      "training------39300, loss: 0.622365\n",
      "batch 39400: average train_loss=1.688244 \n",
      "training------39400, loss: 0.620965\n",
      "batch 39500: average train_loss=1.687667 \n",
      "training------39500, loss: 0.618957\n",
      "batch 39600: average train_loss=1.686581 \n",
      "training------39600, loss: 0.616926\n",
      "batch 39700: average train_loss=1.685392 \n",
      "training------39700, loss: 0.615412\n",
      "batch 39800: average train_loss=1.683625 \n",
      "training------39800, loss: 0.614281\n",
      "batch 39900: average train_loss=1.681389 \n",
      "training------39900, loss: 0.613445\n",
      "batch 40000: average train_loss=1.678152 \n",
      "training------40000, loss: 0.612523\n",
      "batch 40100: average train_loss=1.675730 \n",
      "training------40100, loss: 0.611400\n",
      "batch 40200: average train_loss=1.674418 \n",
      "training------40200, loss: 0.609549\n",
      "batch 40300: average train_loss=1.673828 \n",
      "training------40300, loss: 0.608129\n",
      "batch 40400: average train_loss=1.673892 \n",
      "training------40400, loss: 0.607017\n",
      "batch 40500: average train_loss=1.674599 \n",
      "training------40500, loss: 0.606280\n",
      "batch 40600: average train_loss=1.675601 \n",
      "training------40600, loss: 0.605855\n",
      "batch 40700: average train_loss=1.676481 \n",
      "training------40700, loss: 0.605071\n",
      "batch 40800: average train_loss=1.676524 \n",
      "training------40800, loss: 0.603253\n",
      "batch 40900: average train_loss=1.675502 \n",
      "training------40900, loss: 0.601070\n",
      "batch 41000: average train_loss=1.673882 \n",
      "training------41000, loss: 0.599445\n",
      "batch 41100: average train_loss=1.671373 \n",
      "training------41100, loss: 0.598631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 41200: average train_loss=1.667926 \n",
      "training------41200, loss: 0.598194\n",
      "batch 41300: average train_loss=1.665598 \n",
      "training------41300, loss: 0.597034\n",
      "batch 41400: average train_loss=1.664458 \n",
      "training------41400, loss: 0.595732\n",
      "batch 41500: average train_loss=1.664303 \n",
      "training------41500, loss: 0.594719\n",
      "batch 41600: average train_loss=1.664889 \n",
      "training------41600, loss: 0.594145\n",
      "batch 41700: average train_loss=1.665774 \n",
      "training------41700, loss: 0.593595\n",
      "batch 41800: average train_loss=1.666820 \n",
      "training------41800, loss: 0.593245\n",
      "batch 41900: average train_loss=1.667340 \n",
      "training------41900, loss: 0.591871\n",
      "batch 42000: average train_loss=1.667173 \n",
      "training------42000, loss: 0.590378\n",
      "batch 42100: average train_loss=1.666527 \n",
      "training------42100, loss: 0.588831\n",
      "batch 42200: average train_loss=1.665517 \n",
      "training------42200, loss: 0.587222\n",
      "batch 42300: average train_loss=1.663981 \n",
      "training------42300, loss: 0.586162\n",
      "batch 42400: average train_loss=1.662026 \n",
      "training------42400, loss: 0.585536\n",
      "batch 42800: average train_loss=1.655079 \n",
      "training------42800, loss: 0.581755\n",
      "batch 42900: average train_loss=1.655288 \n",
      "training------42900, loss: 0.580885\n",
      "batch 43000: average train_loss=1.656697 \n",
      "training------43000, loss: 0.580663\n",
      "batch 43100: average train_loss=1.658219 \n",
      "training------43100, loss: 0.580097\n",
      "batch 43200: average train_loss=1.658635 \n",
      "training------43200, loss: 0.578228\n",
      "batch 43300: average train_loss=1.658254 \n",
      "training------43300, loss: 0.575927\n",
      "batch 43400: average train_loss=1.657256 \n",
      "training------43400, loss: 0.574277\n",
      "batch 43500: average train_loss=1.655422 \n",
      "training------43500, loss: 0.573053\n",
      "batch 43600: average train_loss=1.652700 \n",
      "training------43600, loss: 0.572959\n",
      "batch 43700: average train_loss=1.648722 \n",
      "training------43700, loss: 0.573844\n",
      "batch 43800: average train_loss=1.645575 \n",
      "training------43800, loss: 0.573596\n",
      "batch 43900: average train_loss=1.644198 \n",
      "training------43900, loss: 0.572471\n",
      "batch 44000: average train_loss=1.643841 \n",
      "training------44000, loss: 0.571509\n",
      "batch 44100: average train_loss=1.644460 \n",
      "training------44100, loss: 0.571044\n",
      "batch 44200: average train_loss=1.645758 \n",
      "training------44200, loss: 0.570757\n",
      "batch 44300: average train_loss=1.646827 \n",
      "training------44300, loss: 0.569840\n",
      "batch 44400: average train_loss=1.646948 \n",
      "training------44400, loss: 0.567912\n",
      "batch 44500: average train_loss=1.646167 \n",
      "training------44500, loss: 0.565769\n",
      "batch 44600: average train_loss=1.643912 \n",
      "training------44600, loss: 0.564633\n",
      "batch 44700: average train_loss=1.639850 \n",
      "training------44700, loss: 0.565043\n",
      "batch 44800: average train_loss=1.636502 \n",
      "training------44800, loss: 0.563864\n",
      "batch 44900: average train_loss=1.635458 \n",
      "training------44900, loss: 0.562207\n",
      "batch 45000: average train_loss=1.636556 \n",
      "training------45000, loss: 0.561805\n",
      "batch 45100: average train_loss=1.638521 \n",
      "training------45100, loss: 0.561382\n",
      "batch 45200: average train_loss=1.639597 \n",
      "training------45200, loss: 0.559389\n",
      "batch 45300: average train_loss=1.638868 \n",
      "training------45300, loss: 0.556908\n",
      "batch 45400: average train_loss=1.636522 \n",
      "training------45400, loss: 0.555648\n",
      "batch 45500: average train_loss=1.631356 \n",
      "training------45500, loss: 0.556241\n",
      "batch 45600: average train_loss=1.627783 \n",
      "training------45600, loss: 0.554863\n",
      "batch 45700: average train_loss=1.627800 \n",
      "training------45700, loss: 0.553769\n",
      "batch 45800: average train_loss=1.630076 \n",
      "training------45800, loss: 0.553763\n",
      "batch 45900: average train_loss=1.631997 \n",
      "training------45900, loss: 0.552022\n",
      "batch 46000: average train_loss=1.631400 \n",
      "training------46000, loss: 0.549479\n",
      "batch 46100: average train_loss=1.628915 \n",
      "training------46100, loss: 0.548285\n",
      "batch 46200: average train_loss=1.623968 \n",
      "training------46200, loss: 0.548783\n",
      "batch 46300: average train_loss=1.620641 \n",
      "training------46300, loss: 0.547709\n",
      "batch 46400: average train_loss=1.620511 \n",
      "training------46400, loss: 0.546784\n",
      "batch 46500: average train_loss=1.622631 \n",
      "training------46500, loss: 0.546628\n",
      "batch 46600: average train_loss=1.624689 \n",
      "training------46600, loss: 0.545179\n",
      "batch 46700: average train_loss=1.625379 \n",
      "training------46700, loss: 0.542139\n",
      "batch 46800: average train_loss=1.623283 \n",
      "training------46800, loss: 0.540482\n",
      "batch 46900: average train_loss=1.617935 \n",
      "training------46900, loss: 0.541148\n",
      "batch 47000: average train_loss=1.614113 \n",
      "training------47000, loss: 0.540270\n",
      "batch 47100: average train_loss=1.614979 \n",
      "training------47100, loss: 0.539410\n",
      "batch 47200: average train_loss=1.618370 \n",
      "training------47200, loss: 0.539011\n",
      "batch 47300: average train_loss=1.619812 \n",
      "training------47300, loss: 0.537018\n",
      "batch 47400: average train_loss=1.619206 \n",
      "training------47400, loss: 0.534526\n",
      "batch 47500: average train_loss=1.615781 \n",
      "training------47500, loss: 0.534165\n",
      "batch 47600: average train_loss=1.610181 \n",
      "training------47600, loss: 0.534224\n",
      "batch 47700: average train_loss=1.608848 \n",
      "training------47700, loss: 0.532806\n",
      "batch 47800: average train_loss=1.611697 \n",
      "training------47800, loss: 0.532491\n",
      "batch 47900: average train_loss=1.614123 \n",
      "training------47900, loss: 0.531400\n",
      "batch 48000: average train_loss=1.614121 \n",
      "training------48000, loss: 0.529247\n",
      "batch 48100: average train_loss=1.611445 \n",
      "training------48100, loss: 0.528484\n",
      "batch 48200: average train_loss=1.606793 \n",
      "training------48200, loss: 0.528557\n",
      "batch 48300: average train_loss=1.604437 \n",
      "training------48300, loss: 0.527636\n",
      "batch 48400: average train_loss=1.606229 \n",
      "training------48400, loss: 0.527212\n",
      "batch 48500: average train_loss=1.609136 \n",
      "training------48500, loss: 0.525959\n",
      "batch 48600: average train_loss=1.609911 \n",
      "training------48600, loss: 0.522483\n",
      "batch 48700: average train_loss=1.607257 \n",
      "training------48700, loss: 0.521144\n",
      "batch 48800: average train_loss=1.599792 \n",
      "training------48800, loss: 0.523576\n",
      "batch 48900: average train_loss=1.598251 \n",
      "training------48900, loss: 0.522881\n",
      "batch 49000: average train_loss=1.601778 \n",
      "training------49000, loss: 0.522598\n",
      "batch 49100: average train_loss=1.604503 \n",
      "training------49100, loss: 0.520587\n",
      "batch 49200: average train_loss=1.603055 \n",
      "training------49200, loss: 0.517932\n",
      "batch 49300: average train_loss=1.596520 \n",
      "training------49300, loss: 0.519371\n",
      "batch 49400: average train_loss=1.595267 \n",
      "training------49400, loss: 0.518462\n",
      "batch 49500: average train_loss=1.599120 \n",
      "training------49500, loss: 0.517614\n",
      "batch 49600: average train_loss=1.601348 \n",
      "training------49600, loss: 0.514020\n",
      "batch 49700: average train_loss=1.600299 \n",
      "training------49700, loss: 0.510730\n",
      "batch 49800: average train_loss=1.591920 \n",
      "training------49800, loss: 0.514425\n",
      "batch 49900: average train_loss=1.588459 \n",
      "training------49900, loss: 0.514302\n",
      "training------49999, loss: 0.507129\n",
      "End...\n",
      "2021-07-28-02-31-24/\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import util.LAMSGeneration as LAMSFunction\n",
    "import tensorflow.compat.v1 as tf\n",
    "from util.args import argument_parser, model_kwargs, train_kwargs\n",
    "from util.models import CNNModel\n",
    "from util.omniglot import read_dataset\n",
    "from util.train import train\n",
    "from datetime import datetime\n",
    "\n",
    "# # the floor plan image\n",
    "# imageArry = np.array(cv2.imread(\"../imagedata/41-124gray.jpg\", cv2.IMREAD_GRAYSCALE))\n",
    "# # measurement data and id\n",
    "# infoExcel = \"data/MeasureData.xlsx\"\n",
    "# fileData = pd.read_excel(infoExcel,sheet_name=\"data\", header=None)\n",
    "# fileID = pd.read_excel(infoExcel,sheet_name=\"ID\", header=None)\n",
    "saveDir = \"data/Beam\"\n",
    "# lams = LAMSFunction.LAMS(imageArry,fileData,fileID,saveDir)\n",
    "# lams.findVertices()\n",
    "print(\"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now()))\n",
    "datetime_ = \"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now())\n",
    "args = argument_parser(datetime_).parse_args(args=[])\n",
    "\n",
    "train_set=[]\n",
    "for i in range(1,16): \n",
    "    train_set = train_set+read_dataset(saveDir+str(i)+\"/\") #\n",
    "\n",
    "model = CNNModel(**model_kwargs(args))\n",
    "with tf.Session() as sess:\n",
    "    if not args.pretrained:\n",
    "        print('Training...')\n",
    "        train(sess, model, train_set, args.checkpoint, **train_kwargs(args))\n",
    "    else:\n",
    "        print('Restoring from checkpoint...')\n",
    "        tf.train.Saver().restore(sess, tf.train.latest_checkpoint(args.checkpoint))\n",
    "\n",
    "    print('End...')\n",
    "    print(\"{0:%Y-%m-%d-%H-%M-%S/}\".format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57648d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
